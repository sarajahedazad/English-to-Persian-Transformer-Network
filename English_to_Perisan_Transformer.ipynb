{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1Nk1UVFeopOP",
    "outputId": "bbd9e16c-dbeb-45f3-d659-2a73c52d8f10"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |████████████████████████████████| 14.3MB 208kB/s \n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import softmax\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    " \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import softmax\n",
    "device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import spacy\n",
    "from torchtext.datasets import Multi30k\n",
    " \n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import spacy\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import nltk\n",
    " \n",
    "torch.manual_seed(17)\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.optim import Adam, SGD\n",
    "from math import floor\n",
    "from torch.utils.data import Dataset\n",
    "from __future__ import division\n",
    "import math\n",
    "from PIL import Image, ImageOps\n",
    "try:\n",
    "    import accimage\n",
    "except ImportError:\n",
    "    accimage = None\n",
    "import numbers\n",
    "import types\n",
    "import collections\n",
    "from torchvision import transforms\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import torchvision.models as models\n",
    "from matplotlib import image\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "random.seed(9)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.translate.nist_score import corpus_nist\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "\n",
    "\n",
    "import torchtext\n",
    "from torchtext.legacy.datasets import Multi30k\n",
    "from torchtext.legacy.data import Field, BucketIterator\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "import spacy\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "!pip install -q pyonmttok\n",
    "import pyonmttok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "!unzip -uq \"/content/drive/MyDrive/DL_HW4/AFEC-merged-all.zip\" -d \"/content\"\n",
    "!unzip -uq \"/content/drive/MyDrive/DL_HW4/Test.zip\" -d \"/content\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**I used these tutorials aid to write my cide:**\n",
    "\n",
    "https://towardsdatascience.com/byte-pair-encoding-the-dark-horse-of-modern-nlp-eb36c7df4f10 https://www.youtube.com/watch?v=U0s0f995w14 https://medium.com/the-dl/transformers-from-scratch-in-pytorch-8777e346ca51 https://colab.research.google.com/github/bentrevett/pytorch-seq2seq/blob/master/6%20-%20Attention%20is%20All%20You%20Need.ipynb#scrollTo=IGGB4rUy_0zx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "eTTDNHMxK3Ni"
   },
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "  def __init__(self,repeated,tokenizer):\n",
    "    self.num2word={0:'<PAD>',1:'<SOS>',2:'<EOS>',3:'<UNK>'}\n",
    "    self.word2num={b:a for a,b in self.num2word.items()}\n",
    "    self.tokenizer=tokenizer\n",
    "    self.repeated=repeated\n",
    " \n",
    " \n",
    "  def preprocess(self,sentence):\n",
    "    tokenized_sentence=self.tokenizer.tokenize(sentence)[0]\n",
    "    return tokenized_sentence \n",
    " \n",
    "  def Update_Vocabulary(self,lst_sentences):\n",
    "    repeated_words={} \n",
    "    for sentence in lst_sentences:\n",
    "      words=self.preprocess(sentence)\n",
    "      for word in words:\n",
    "        if word in repeated_words:\n",
    "          repeated_words[word]=repeated_words[word]+1\n",
    "        else:\n",
    "          repeated_words[word]=1\n",
    "        if repeated_words[word]==self.repeated:\n",
    "          new_num=len(self.num2word)\n",
    "          self.num2word[new_num]=word\n",
    "          self.word2num[word]=new_num\n",
    "  \n",
    "  def sentence_to_numbers(self,sentence): \n",
    "    numbers=[]\n",
    "    words=self.preprocess(sentence)\n",
    "    for word in words:\n",
    "      if word in self.word2num:\n",
    "        number=self.word2num[word]\n",
    "      else:\n",
    "        number=self.word2num['<UNK>']\n",
    "      numbers.append(number)\n",
    "    return numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "XvDK4eZT7L2a"
   },
   "outputs": [],
   "source": [
    "english_path='/content/AFEC-merged.en'\n",
    "farsi_path='/content/AFEC-merged.fa'\n",
    "\n",
    "args={\"mode\":\"aggressive\",\"case_feature\":True,\"joiner_annotate\":True,\"joiner\":\"_\",\"preserve_placeholders\":True,\"preserve_segmented_tokens\":True}\n",
    "tokenizer_en=pyonmttok.Tokenizer(**args)\n",
    "learner_en=pyonmttok.BPELearner(tokenizer_en,symbols=32000)\n",
    "learner_en.ingest_file(english_path)\n",
    "tokenizer_en=learner_en.learn(\"model-32k\")\n",
    "\n",
    "tokenizer_fa=pyonmttok.Tokenizer(**args)\n",
    "learner_fa=pyonmttok.BPELearner(tokenizer_fa,symbols=32000)\n",
    "learner_fa.ingest_file(farsi_path)\n",
    "tokenizer_fa=learner_fa.learn(\"model-32k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "KndLZ0WQAosG"
   },
   "outputs": [],
   "source": [
    "english_path='/content/AFEC-merged.en'\n",
    "farsi_path='/content/AFEC-merged.fa'\n",
    "\n",
    "\n",
    "f=open(english_path,'r')\n",
    "en_lines=f.readlines()\n",
    "f.close()\n",
    "\n",
    "f=open(farsi_path,'r')\n",
    "fa_lines=f.readlines()\n",
    "f.close()\n",
    "\n",
    "English_Vocabulary=Vocabulary(5,tokenizer_en)\n",
    "English_Vocabulary.Update_Vocabulary(en_lines)\n",
    "\n",
    "Farsi_Vocabulary=Vocabulary(5,tokenizer_fa)\n",
    "Farsi_Vocabulary.Update_Vocabulary(fa_lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "cl-uGKcNR7Ek"
   },
   "outputs": [],
   "source": [
    "def Positional_Encoder(x):\n",
    "    x_size=x.size()\n",
    "\n",
    "    pos=torch.arange(x_size[1],dtype=torch.float,device=device) #place of each token in a sequence\n",
    " \n",
    "    embd_feature_num=torch.arange(x_size[2],dtype=torch.float,device=device)\n",
    "    embd_feature_num=embd_feature_num.reshape(1,1,-1)\n",
    "    i_s=embd_feature_num//2\n",
    " \n",
    "    angle=pos.reshape(1,-1,1)/(10000**(2*i_s/x_size[2]))\n",
    "    PE=torch.where(embd_feature_num.long()%2==0,torch.sin(angle),torch.cos(angle))\n",
    "\n",
    "    PE.required_grad=False\n",
    "    return PE\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "U8N9endPRB9c"
   },
   "outputs": [],
   "source": [
    "class Multi_Head_Attention(nn.Module):\n",
    "  def __init__(self,embed_size,num_heads,dim_key,dim_value,dim_inner):\n",
    "    super(Multi_Head_Attention,self).__init__()\n",
    "    self.embed_size=embed_size\n",
    "    self.num_heads=num_heads\n",
    "    self.dim_key=dim_key\n",
    "    self.dim_value=dim_value\n",
    "    self.dim_inner=dim_inner\n",
    "\n",
    "    self.query_linear=nn.Linear(self.embed_size,self.num_heads*self.dim_key)\n",
    "    self.key_linear=nn.Linear(self.embed_size,self.num_heads*self.dim_key)\n",
    "    self.value_linear=nn.Linear(self.embed_size,self.num_heads*self.dim_value)\n",
    "\n",
    "    self.dropout_layer=nn.Dropout(0.1)\n",
    "\n",
    "    self.fc_layer=nn.Linear(self.num_heads*self.dim_value,self.embed_size)\n",
    "\n",
    " \n",
    "  def scaled_dot_product_attention(self,q,k,v,en_mask):\n",
    "    x=q.matmul(k.transpose(-2,-1))\n",
    "    x=x/(self.dim_key**0.5)\n",
    "\n",
    "    if en_mask!=None:\n",
    "      x=x.masked_fill(en_mask==1,float(\"-inf\"))\n",
    "    attention=softmax(x,dim=-1)\n",
    "    x=self.dropout_layer(attention)\n",
    "    x=x.matmul(v)\n",
    "    return x\n",
    " \n",
    "    \n",
    "  def forward(self,Q,K,V,en_mask=None):\n",
    " \n",
    "    batch_size=Q.size(0)\n",
    "\n",
    "    q_len=Q.size(1)\n",
    "    k_len=K.size(1)\n",
    "    v_len=V.size(1)\n",
    "\n",
    "    Q=self.query_linear(Q)\n",
    "    K=self.key_linear(K)\n",
    "    V=self.value_linear(V)\n",
    "\n",
    "    #splitting to heads\n",
    "    Q=Q.view(batch_size,q_len,self.num_heads,self.dim_key)\n",
    "    Q=Q.transpose(1,2)\n",
    "\n",
    "    K=K.view(batch_size,k_len,self.num_heads,self.dim_key)\n",
    "    K=K.transpose(1,2)\n",
    "\n",
    "    V=V.view(batch_size,v_len,self.num_heads,self.dim_value)\n",
    "    V=V.transpose(1,2)\n",
    "\n",
    "    out=self.scaled_dot_product_attention(Q,K,V,en_mask)\n",
    "    \"\"\" \n",
    "\n",
    "    take good care of here\n",
    "\n",
    "    \"\"\"\n",
    "    out=out.permute(0, 2, 1, 3).contiguous()\n",
    "\n",
    "    out=out.view(batch_size,-1,self.num_heads*self.dim_value)\n",
    "\n",
    "    out=self.fc_layer(out)\n",
    "\n",
    "\n",
    "    return out\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "EFfFWRKbhLwO"
   },
   "outputs": [],
   "source": [
    "class POS_FFN(nn.Module):\n",
    "  def __init__(self,embed_size,dim_inner):\n",
    "    super(POS_FFN,self).__init__()\n",
    "    self.embed_size=embed_size\n",
    "    self.dim_inner=dim_inner\n",
    " \n",
    "    self.linear1=nn.Linear(self.embed_size,self.dim_inner)\n",
    "    self.linear2=nn.Linear(self.dim_inner,self.embed_size)\n",
    "    self.dropout_layer=nn.Dropout(0.1)\n",
    " \n",
    "  def forward(self,x):\n",
    "    x=self.linear1(x)\n",
    "    x=torch.relu(x)\n",
    "    x=self.dropout_layer(x)\n",
    "\n",
    "    x=self.linear2(x)\n",
    " \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "noQToEFDiSRX"
   },
   "outputs": [],
   "source": [
    "class Encoder_Layer(nn.Module):\n",
    "  def __init__(self,embed_size,num_heads,dim_key,dim_value,dim_inner):\n",
    "    super(Encoder_Layer,self).__init__()\n",
    "    self.multi_head_attention=Multi_Head_Attention(embed_size,num_heads,dim_key,dim_value,dim_inner)\n",
    "    self.norm1=nn.LayerNorm(embed_size)\n",
    "    self.feedforward=POS_FFN(embed_size,dim_inner)\n",
    "    self.norm2=nn.LayerNorm(embed_size)\n",
    "    self.dropout_layer=nn.Dropout(0.1)\n",
    "\n",
    "\n",
    "  def forward(self,x,x_mask):\n",
    "    x_out=self.multi_head_attention(x,x,x,x_mask)\n",
    "    x_out=self.dropout_layer(x_out)\n",
    "    x=self.norm1(x+x_out)\n",
    " \n",
    "    x_out=self.feedforward(x)\n",
    "    x_out=self.dropout_layer(x_out)\n",
    "    x=self.norm2(x+x_out)\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "p2EqVIdVptUA"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "  def __init__(self,embed_size,num_heads,dim_key,dim_value,dim_inner,src_vocab_size):\n",
    "    super(Encoder,self).__init__()\n",
    "    self.scale=embed_size**0.5\n",
    "    self.embedding=nn.Embedding(src_vocab_size,embed_size)\n",
    "    self.encoder_layer1=Encoder_Layer(embed_size,num_heads,dim_key,dim_value,dim_inner)\n",
    "    self.encoder_layer2=Encoder_Layer(embed_size,num_heads,dim_key,dim_value,dim_inner)\n",
    "    self.encoder_layer3=Encoder_Layer(embed_size,num_heads,dim_key,dim_value,dim_inner)\n",
    "    self.dropout_layer=nn.Dropout(0.1)  \n",
    "\n",
    "\n",
    "  def forward(self,x,x_mask):\n",
    "    x=self.embedding(x)\n",
    "    x=x*self.scale+Positional_Encoder(x)\n",
    "    x=self.dropout_layer(x)\n",
    "    x=self.encoder_layer1(x,x_mask)\n",
    "    x=self.encoder_layer2(x,x_mask)\n",
    "    x=self.encoder_layer3(x,x_mask)\n",
    "\n",
    "    return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "CbAod-bxvZ2v"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "  def __init__(self,embed_size,num_heads,dim_inner,layer_num,tgt_vocab_size):\n",
    "    super(Decoder,self).__init__()\n",
    "    self.scale=embed_size**0.5\n",
    "    self.embedding=nn.Embedding(tgt_vocab_size,embed_size)\n",
    "    self.decoder_layer=nn.TransformerDecoderLayer(embed_size,num_heads,dim_feedforward=dim_inner,batch_first=True)\n",
    "    self.decoder_whole=nn.TransformerDecoder(self.decoder_layer,layer_num)\n",
    "    self.dropout_layer=nn.Dropout(0.1)\n",
    "\n",
    "  def forward(self,tgt,src,mask_dic):\n",
    "    tgt=self.embedding(tgt)\n",
    "    tgt=tgt*self.scale+Positional_Encoder(tgt)\n",
    "    tgt=self.dropout_layer(tgt)\n",
    "    out=self.decoder_whole(tgt,src,**mask_dic)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "vMIlJ85cOyjD"
   },
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "  def __init__(self,embed_size,num_heads,dim_key,dim_value,dim_inner,layer_num,src_vocab_size,tgt_vocab_size):\n",
    "    super(Transformer,self).__init__()\n",
    "    self.encoder=Encoder(embed_size,num_heads,dim_key,dim_value,dim_inner,src_vocab_size)\n",
    "    self.decoder=Decoder(embed_size,num_heads,dim_inner,layer_num,tgt_vocab_size)\n",
    "    self.fc=nn.Linear(embed_size,tgt_vocab_size)\n",
    "\n",
    "    \n",
    "    #masks creditted to \n",
    "    #https://colab.research.google.com/github/bentrevett/pytorch-seq2seq/blob/master/6%20-%20Attention%20is%20All%20You%20Need.ipynb#scrollTo=IGGB4rUy_0zx\n",
    "\n",
    "  def build_source_mask(self,source):\n",
    "    source_mask=source ==0\n",
    "    return source_mask.unsqueeze(1).unsqueeze(2).to(device)\n",
    "\n",
    "  def build_pad_mask(self,seq):\n",
    "    the_mask=seq==0\n",
    "    return the_mask.to(device)\n",
    "\n",
    "  def build_target_mask(self,target):\n",
    "    sz=target.size(1)\n",
    "    trg_mask=(torch.triu(torch.ones(sz,sz))==1).transpose(0,1)\n",
    "    trg_mask=trg_mask.float().masked_fill(trg_mask==0,float('-inf')).masked_fill(trg_mask==1,float(0.0))\n",
    "    return trg_mask.to(device)\n",
    "\n",
    "  def forward(self,source,target):\n",
    "\n",
    "    source_mask=self.build_source_mask(source)\n",
    "    \n",
    "\n",
    "    memory_p_mask=self.build_pad_mask(source)\n",
    "    target_mask=self.build_target_mask(target)\n",
    "    target_p_mask=self.build_pad_mask(target)\n",
    "\n",
    "    source=self.encoder(source,source_mask)\n",
    "    \n",
    "    mask_dic={'tgt_mask':target_mask,'memory_mask':None,'tgt_key_padding_mask':target_p_mask, 'memory_key_padding_mask':memory_p_mask}\n",
    "\n",
    "    out=self.decoder(target,source,mask_dic)\n",
    "\n",
    "    out=self.fc(out)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "fVUV6gBc-sc6"
   },
   "outputs": [],
   "source": [
    "class Customized_Dataset(Dataset):\n",
    "  def __init__(self,en_list,fa_list,en_vocab,fa_vocab):\n",
    " \n",
    "    self.en_list=en_list\n",
    "    self.fa_list=fa_list\n",
    " \n",
    "    self.en_vocab=en_vocab\n",
    "    self.fa_vocab=fa_vocab\n",
    " \n",
    " \n",
    "  def __len__(self):\n",
    "    L=len(self.en_list)\n",
    "    return L\n",
    " \n",
    "  def __getitem__(self,index):\n",
    "    \n",
    "    en_sen=self.en_list[index]\n",
    "    en2num=[1]+self.en_vocab.sentence_to_numbers(en_sen)+[2]\n",
    "    en2num=torch.tensor(en2num)\n",
    " \n",
    "    fa_sen=self.fa_list[index]\n",
    "    fa2num=[1]+self.fa_vocab.sentence_to_numbers(fa_sen)+[2]\n",
    "    fa2num=torch.tensor(fa2num)\n",
    " \n",
    "    return en2num,fa2num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "yaxNQkSM-vYH"
   },
   "outputs": [],
   "source": [
    " class PaddingBatches:\n",
    "  def __call__(self,mini_batch):\n",
    "    source_lst=[]\n",
    "    target_lst=[]\n",
    "    for s,t in mini_batch:\n",
    "      source_lst.append(s)\n",
    "      target_lst.append(t)\n",
    " \n",
    "    source_lst=pad_sequence(source_lst,padding_value=0,batch_first=True)\n",
    "   \n",
    "    target_lst=pad_sequence(target_lst,padding_value=0,batch_first=True)\n",
    "    \n",
    " \n",
    "    return source_lst,target_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BMTVz2zoJyQT",
    "outputId": "7cf34c51-3ad1-45ab-89fa-204f4d1fe5c8"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    }
   ],
   "source": [
    "Batch_Size=32\n",
    "embed_size=256\n",
    "num_heads=8\n",
    "dim_key=64\n",
    "dim_value=64\n",
    "dim_inner=1024\n",
    "layer_num=3\n",
    "src_vocab_size=len(English_Vocabulary.word2num)\n",
    "tgt_vocab_size=len(Farsi_Vocabulary.word2num)\n",
    "\n",
    "learning_rate=0.0001\n",
    "Betas_range=(0.9,0.98)\n",
    "eps_value=1e-9\n",
    "\n",
    "x_train,x_val,y_train,y_val=train_test_split(en_lines,fa_lines, test_size=0.1, random_state=42)\n",
    " \n",
    "\n",
    "params={'batch_size': Batch_Size,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 6,'collate_fn':PaddingBatches()}\n",
    " \n",
    "dataset=Customized_Dataset(x_train,y_train,English_Vocabulary,Farsi_Vocabulary)\n",
    "train_generator=DataLoader(dataset,**params)\n",
    "\n",
    "\n",
    "params_val={'batch_size': 10,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 6,'collate_fn':PaddingBatches()}\n",
    "\n",
    "dataset_val=Customized_Dataset(x_val,y_val,English_Vocabulary,Farsi_Vocabulary)\n",
    "val_generator=DataLoader(dataset_val,**params_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nC5UfIHj_Aje",
    "outputId": "10a5dda9-9a17-4755-f3cd-ac00575cf9b6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n",
      "/usr/local/lib/python3.7/dist-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  150] train loss: 8.120244\n",
      "[1,  150] val loss: 7.006501\n",
      "[1,  300] train loss: 6.753753\n",
      "[1,  300] val loss: 6.562801\n",
      "[1,  450] train loss: 6.558768\n",
      "[1,  450] val loss: 6.491592\n",
      "[1,  600] train loss: 6.415642\n",
      "[1,  600] val loss: 6.287014\n",
      "[1,  750] train loss: 6.292838\n",
      "[1,  750] val loss: 6.396667\n",
      "[1,  900] train loss: 6.205105\n",
      "[1,  900] val loss: 6.126094\n",
      "[1, 1050] train loss: 6.127204\n",
      "[1, 1050] val loss: 6.088767\n",
      "[1, 1200] train loss: 6.045751\n",
      "[1, 1200] val loss: 6.030107\n",
      "[1, 1350] train loss: 6.006832\n",
      "[1, 1350] val loss: 5.934592\n",
      "[1, 1500] train loss: 5.971934\n",
      "[1, 1500] val loss: 6.063160\n",
      "[1, 1650] train loss: 5.894015\n",
      "[1, 1650] val loss: 5.767847\n",
      "[1, 1800] train loss: 5.854511\n",
      "[1, 1800] val loss: 5.997239\n",
      "[1, 1950] train loss: 5.821502\n",
      "[1, 1950] val loss: 5.635509\n",
      "[1, 2100] train loss: 5.755910\n",
      "[1, 2100] val loss: 5.829309\n",
      "[1, 2250] train loss: 5.754061\n",
      "[1, 2250] val loss: 5.708641\n",
      "[1, 2400] train loss: 5.731368\n",
      "[1, 2400] val loss: 5.781079\n",
      "[1, 2550] train loss: 5.691239\n",
      "[1, 2550] val loss: 5.556039\n",
      "[1, 2700] train loss: 5.658352\n",
      "[1, 2700] val loss: 5.790997\n",
      "[1, 2850] train loss: 5.638040\n",
      "[1, 2850] val loss: 5.440364\n",
      "[1, 3000] train loss: 5.593148\n",
      "[1, 3000] val loss: 5.553387\n",
      "[1, 3150] train loss: 5.558050\n",
      "[1, 3150] val loss: 5.434164\n",
      "[1, 3300] train loss: 5.542649\n",
      "[1, 3300] val loss: 5.642330\n",
      "[1, 3450] train loss: 5.490017\n",
      "[1, 3450] val loss: 5.498837\n",
      "[1, 3600] train loss: 5.481160\n",
      "[1, 3600] val loss: 5.453130\n",
      "[1, 3750] train loss: 5.475033\n",
      "[1, 3750] val loss: 5.485250\n",
      "[1, 3900] train loss: 5.460554\n",
      "[1, 3900] val loss: 5.385550\n",
      "[1, 4050] train loss: 5.434373\n",
      "[1, 4050] val loss: 5.261502\n",
      "[1, 4200] train loss: 5.428926\n",
      "[1, 4200] val loss: 5.300231\n",
      "[1, 4350] train loss: 5.405164\n",
      "[1, 4350] val loss: 5.470473\n",
      "[1, 4500] train loss: 5.368267\n",
      "[1, 4500] val loss: 5.336889\n",
      "[1, 4650] train loss: 5.342154\n",
      "[1, 4650] val loss: 5.499782\n",
      "[1, 4800] train loss: 5.341771\n",
      "[1, 4800] val loss: 5.247532\n",
      "[1, 4950] train loss: 5.295167\n",
      "[1, 4950] val loss: 5.451392\n",
      "[1, 5100] train loss: 5.296565\n",
      "[1, 5100] val loss: 5.417711\n",
      "[1, 5250] train loss: 5.287915\n",
      "[1, 5250] val loss: 5.336854\n",
      "[1, 5400] train loss: 5.248149\n",
      "[1, 5400] val loss: 5.175703\n",
      "[1, 5550] train loss: 5.258310\n",
      "[1, 5550] val loss: 5.344623\n",
      "[1, 5700] train loss: 5.234318\n",
      "[1, 5700] val loss: 5.134241\n",
      "[1, 5850] train loss: 5.244222\n",
      "[1, 5850] val loss: 5.337610\n",
      "[1, 6000] train loss: 5.211475\n",
      "[1, 6000] val loss: 5.189746\n",
      "[1, 6150] train loss: 5.199305\n",
      "[1, 6150] val loss: 5.161120\n",
      "[1, 6300] train loss: 5.194527\n",
      "[1, 6300] val loss: 5.114672\n",
      "[1, 6450] train loss: 5.139457\n",
      "[1, 6450] val loss: 5.254708\n",
      "[1, 6600] train loss: 5.185986\n",
      "[1, 6600] val loss: 5.001555\n",
      "[1, 6750] train loss: 5.152713\n",
      "[1, 6750] val loss: 5.052682\n",
      "[1, 6900] train loss: 5.141122\n",
      "[1, 6900] val loss: 5.100577\n",
      "[1, 7050] train loss: 5.120685\n",
      "[1, 7050] val loss: 5.068709\n",
      "[1, 7200] train loss: 5.106864\n",
      "[1, 7200] val loss: 5.266553\n",
      "[1, 7350] train loss: 5.098899\n",
      "[1, 7350] val loss: 5.230453\n",
      "[1, 7500] train loss: 5.108531\n",
      "[1, 7500] val loss: 5.014639\n",
      "[1, 7650] train loss: 5.079024\n",
      "[1, 7650] val loss: 5.182989\n",
      "[1, 7800] train loss: 5.037762\n",
      "[1, 7800] val loss: 5.127071\n",
      "[1, 7950] train loss: 5.046114\n",
      "[1, 7950] val loss: 5.004267\n",
      "[1, 8100] train loss: 5.032502\n",
      "[1, 8100] val loss: 5.024053\n",
      "[1, 8250] train loss: 5.042964\n",
      "[1, 8250] val loss: 5.086396\n",
      "[1, 8400] train loss: 5.038514\n",
      "[1, 8400] val loss: 5.342534\n",
      "[1, 8550] train loss: 5.015004\n",
      "[1, 8550] val loss: 5.078725\n",
      "[1, 8700] train loss: 4.991955\n",
      "[1, 8700] val loss: 4.883505\n",
      "[1, 8850] train loss: 4.994485\n",
      "[1, 8850] val loss: 4.733592\n",
      "[1, 9000] train loss: 4.990845\n",
      "[1, 9000] val loss: 4.810010\n",
      "[1, 9150] train loss: 4.974857\n",
      "[1, 9150] val loss: 4.832246\n",
      "[1, 9300] train loss: 4.944183\n",
      "[1, 9300] val loss: 4.918701\n",
      "[1, 9450] train loss: 4.938206\n",
      "[1, 9450] val loss: 5.023827\n",
      "[1, 9600] train loss: 4.959055\n",
      "[1, 9600] val loss: 4.837217\n",
      "[1, 9750] train loss: 4.925392\n",
      "[1, 9750] val loss: 4.949336\n",
      "[1, 9900] train loss: 4.933129\n",
      "[1, 9900] val loss: 5.004545\n",
      "[1,10050] train loss: 4.951715\n",
      "[1,10050] val loss: 4.795099\n",
      "[1,10200] train loss: 4.922054\n",
      "[1,10200] val loss: 4.760347\n",
      "[1,10350] train loss: 4.935789\n",
      "[1,10350] val loss: 4.769069\n",
      "[1,10500] train loss: 4.901127\n",
      "[1,10500] val loss: 4.808333\n",
      "[1,10650] train loss: 4.860108\n",
      "[1,10650] val loss: 4.742059\n",
      "[1,10800] train loss: 4.874266\n",
      "[1,10800] val loss: 4.989710\n",
      "[1,10950] train loss: 4.879353\n",
      "[1,10950] val loss: 4.810984\n",
      "[1,11100] train loss: 4.863530\n",
      "[1,11100] val loss: 4.968982\n",
      "[1,11250] train loss: 4.862304\n",
      "[1,11250] val loss: 4.687987\n",
      "[1,11400] train loss: 4.827655\n",
      "[1,11400] val loss: 4.917614\n",
      "[1,11550] train loss: 4.846544\n",
      "[1,11550] val loss: 4.864159\n",
      "[1,11700] train loss: 4.851930\n",
      "[1,11700] val loss: 4.802762\n",
      "[1,11850] train loss: 4.822625\n",
      "[1,11850] val loss: 4.857621\n",
      "[1,12000] train loss: 4.833377\n",
      "[1,12000] val loss: 4.805658\n",
      "[1,12150] train loss: 4.838837\n",
      "[1,12150] val loss: 4.653402\n",
      "[1,12300] train loss: 4.817653\n",
      "[1,12300] val loss: 4.715839\n",
      "[1,12450] train loss: 4.760489\n",
      "[1,12450] val loss: 4.698256\n",
      "[1,12600] train loss: 4.801341\n",
      "[1,12600] val loss: 4.749695\n",
      "[1,12750] train loss: 4.788580\n",
      "[1,12750] val loss: 4.799440\n",
      "[1,12900] train loss: 4.809218\n",
      "[1,12900] val loss: 4.599660\n",
      "[1,13050] train loss: 4.776692\n",
      "[1,13050] val loss: 4.691010\n",
      "[1,13200] train loss: 4.748585\n",
      "[1,13200] val loss: 4.537579\n",
      "[1,13350] train loss: 4.755668\n",
      "[1,13350] val loss: 4.581698\n",
      "[1,13500] train loss: 4.760111\n",
      "[1,13500] val loss: 4.804754\n",
      "[1,13650] train loss: 4.769459\n",
      "[1,13650] val loss: 4.627622\n",
      "[1,13800] train loss: 4.740057\n",
      "[1,13800] val loss: 4.718661\n",
      "[1,13950] train loss: 4.717048\n",
      "[1,13950] val loss: 4.747571\n",
      "[1,14100] train loss: 4.736364\n",
      "[1,14100] val loss: 4.741830\n",
      "[1,14250] train loss: 4.714252\n",
      "[1,14250] val loss: 4.906868\n",
      "[1,14400] train loss: 4.723798\n",
      "[1,14400] val loss: 4.778921\n",
      "[1,14550] train loss: 4.692788\n",
      "[1,14550] val loss: 4.828836\n",
      "[1,14700] train loss: 4.676708\n",
      "[1,14700] val loss: 4.663697\n",
      "[1,14850] train loss: 4.687659\n",
      "[1,14850] val loss: 4.492227\n",
      "[1,15000] train loss: 4.679563\n",
      "[1,15000] val loss: 4.702080\n",
      "[1,15150] train loss: 4.713477\n",
      "[1,15150] val loss: 4.782679\n",
      "[1,15300] train loss: 4.681885\n",
      "[1,15300] val loss: 4.884691\n",
      "[1,15450] train loss: 4.656710\n",
      "[1,15450] val loss: 4.659251\n",
      "[1,15600] train loss: 4.664885\n",
      "[1,15600] val loss: 4.638003\n",
      "[1,15750] train loss: 4.693434\n",
      "[1,15750] val loss: 4.635208\n",
      "[1,15900] train loss: 4.628845\n",
      "[1,15900] val loss: 4.769627\n",
      "[1,16050] train loss: 4.661176\n",
      "[1,16050] val loss: 4.503232\n",
      "[1,16200] train loss: 4.670043\n",
      "[1,16200] val loss: 4.541670\n",
      "[1,16350] train loss: 4.629072\n",
      "[1,16350] val loss: 4.551484\n",
      "[1,16500] train loss: 4.659365\n",
      "[1,16500] val loss: 4.521597\n",
      "[1,16650] train loss: 4.667209\n",
      "[1,16650] val loss: 4.779224\n",
      "[1,16800] train loss: 4.626311\n",
      "[1,16800] val loss: 4.598818\n",
      "[1,16950] train loss: 4.628485\n",
      "[1,16950] val loss: 4.598352\n",
      "[1,17100] train loss: 4.618368\n",
      "[1,17100] val loss: 4.816564\n",
      "[1,17250] train loss: 4.629863\n",
      "[1,17250] val loss: 4.359803\n",
      "[1,17400] train loss: 4.614252\n",
      "[1,17400] val loss: 4.698597\n",
      "[1,17550] train loss: 4.630665\n",
      "[1,17550] val loss: 4.589002\n",
      "[1,17700] train loss: 4.608161\n",
      "[1,17700] val loss: 4.591830\n",
      "[1,17850] train loss: 4.620104\n",
      "[1,17850] val loss: 4.810880\n",
      "[1,18000] train loss: 4.586964\n",
      "[1,18000] val loss: 4.687840\n",
      "[1,18150] train loss: 4.609073\n",
      "[1,18150] val loss: 4.702030\n",
      "[1,18300] train loss: 4.566101\n",
      "[1,18300] val loss: 4.525935\n",
      "[1,18450] train loss: 4.572787\n",
      "[1,18450] val loss: 4.689414\n",
      "[1,18600] train loss: 4.551552\n",
      "[1,18600] val loss: 4.562481\n",
      "[1,18750] train loss: 4.577904\n",
      "[1,18750] val loss: 4.529449\n",
      "[1,18900] train loss: 4.567318\n",
      "[1,18900] val loss: 4.608880\n",
      "[1,19050] train loss: 4.586547\n",
      "[1,19050] val loss: 4.543456\n",
      "[1,19200] train loss: 4.576381\n",
      "[1,19200] val loss: 4.678706\n",
      "[2,  150] train loss: 4.524588\n",
      "[2,  150] val loss: 4.551554\n",
      "[2,  300] train loss: 4.516018\n",
      "[2,  300] val loss: 4.670342\n",
      "[2,  450] train loss: 4.533890\n",
      "[2,  450] val loss: 4.538804\n",
      "[2,  600] train loss: 4.547344\n",
      "[2,  600] val loss: 4.577113\n",
      "[2,  750] train loss: 4.508808\n",
      "[2,  750] val loss: 4.497138\n",
      "[2,  900] train loss: 4.497434\n",
      "[2,  900] val loss: 4.370593\n",
      "[2, 1050] train loss: 4.496914\n",
      "[2, 1050] val loss: 4.587695\n",
      "[2, 1200] train loss: 4.508346\n",
      "[2, 1200] val loss: 4.569525\n",
      "[2, 1350] train loss: 4.515401\n",
      "[2, 1350] val loss: 4.452202\n",
      "[2, 1500] train loss: 4.503383\n",
      "[2, 1500] val loss: 4.447413\n",
      "[2, 1650] train loss: 4.479491\n",
      "[2, 1650] val loss: 4.561800\n",
      "[2, 1800] train loss: 4.482007\n",
      "[2, 1800] val loss: 4.349277\n",
      "[2, 1950] train loss: 4.492074\n",
      "[2, 1950] val loss: 4.331410\n",
      "[2, 2100] train loss: 4.511783\n",
      "[2, 2100] val loss: 4.480253\n",
      "[2, 2250] train loss: 4.494363\n",
      "[2, 2250] val loss: 4.335946\n",
      "[2, 2400] train loss: 4.517100\n",
      "[2, 2400] val loss: 4.495840\n",
      "[2, 2550] train loss: 4.455269\n",
      "[2, 2550] val loss: 4.485585\n",
      "[2, 2700] train loss: 4.455760\n",
      "[2, 2700] val loss: 4.297994\n",
      "[2, 2850] train loss: 4.467490\n",
      "[2, 2850] val loss: 4.419251\n",
      "[2, 3000] train loss: 4.433304\n",
      "[2, 3000] val loss: 4.541771\n",
      "[2, 3150] train loss: 4.461980\n",
      "[2, 3150] val loss: 4.392937\n",
      "[2, 3300] train loss: 4.493201\n",
      "[2, 3300] val loss: 4.392610\n",
      "[2, 3450] train loss: 4.448731\n",
      "[2, 3450] val loss: 4.527830\n",
      "[2, 3600] train loss: 4.481436\n",
      "[2, 3600] val loss: 4.446093\n",
      "[2, 3750] train loss: 4.456059\n",
      "[2, 3750] val loss: 4.380456\n",
      "[2, 3900] train loss: 4.452709\n",
      "[2, 3900] val loss: 4.728073\n",
      "[2, 4050] train loss: 4.438317\n",
      "[2, 4050] val loss: 4.651668\n",
      "[2, 4200] train loss: 4.460378\n",
      "[2, 4200] val loss: 4.558554\n",
      "[2, 4350] train loss: 4.415494\n",
      "[2, 4350] val loss: 4.376276\n",
      "[2, 4500] train loss: 4.411008\n",
      "[2, 4500] val loss: 4.422138\n",
      "[2, 4650] train loss: 4.424013\n",
      "[2, 4650] val loss: 4.287489\n",
      "[2, 4800] train loss: 4.420491\n",
      "[2, 4800] val loss: 4.643871\n",
      "[2, 4950] train loss: 4.418489\n",
      "[2, 4950] val loss: 4.424285\n",
      "[2, 5100] train loss: 4.449257\n",
      "[2, 5100] val loss: 4.542819\n",
      "[2, 5250] train loss: 4.414597\n",
      "[2, 5250] val loss: 4.265236\n",
      "[2, 5400] train loss: 4.427107\n",
      "[2, 5400] val loss: 4.437442\n",
      "[2, 5550] train loss: 4.418978\n",
      "[2, 5550] val loss: 4.430839\n",
      "[2, 5700] train loss: 4.441719\n",
      "[2, 5700] val loss: 4.389396\n",
      "[2, 5850] train loss: 4.421366\n",
      "[2, 5850] val loss: 4.358967\n",
      "[2, 6000] train loss: 4.402710\n",
      "[2, 6000] val loss: 4.279494\n",
      "[2, 6150] train loss: 4.380159\n",
      "[2, 6150] val loss: 4.674646\n",
      "[2, 6300] train loss: 4.391765\n",
      "[2, 6300] val loss: 4.526239\n",
      "[2, 6450] train loss: 4.410470\n",
      "[2, 6450] val loss: 4.535086\n",
      "[2, 6600] train loss: 4.405357\n",
      "[2, 6600] val loss: 4.353178\n",
      "[2, 6750] train loss: 4.377390\n",
      "[2, 6750] val loss: 4.390230\n",
      "[2, 6900] train loss: 4.407811\n",
      "[2, 6900] val loss: 4.590006\n",
      "[2, 7050] train loss: 4.386232\n",
      "[2, 7050] val loss: 4.378168\n",
      "[2, 7200] train loss: 4.385205\n",
      "[2, 7200] val loss: 4.550440\n",
      "[2, 7350] train loss: 4.353750\n",
      "[2, 7350] val loss: 4.403873\n",
      "[2, 7500] train loss: 4.361031\n",
      "[2, 7500] val loss: 4.681231\n",
      "[2, 7650] train loss: 4.351855\n",
      "[2, 7650] val loss: 4.149979\n",
      "[2, 7800] train loss: 4.395601\n",
      "[2, 7800] val loss: 4.148650\n",
      "[2, 7950] train loss: 4.353687\n",
      "[2, 7950] val loss: 4.378513\n",
      "[2, 8100] train loss: 4.356377\n",
      "[2, 8100] val loss: 4.319059\n",
      "[2, 8250] train loss: 4.363312\n",
      "[2, 8250] val loss: 4.503430\n",
      "[2, 8400] train loss: 4.341114\n",
      "[2, 8400] val loss: 4.427991\n",
      "[2, 8550] train loss: 4.361398\n",
      "[2, 8550] val loss: 4.568296\n",
      "[2, 8700] train loss: 4.368260\n",
      "[2, 8700] val loss: 4.258517\n",
      "[2, 8850] train loss: 4.389067\n",
      "[2, 8850] val loss: 4.418880\n",
      "[2, 9000] train loss: 4.344622\n",
      "[2, 9000] val loss: 4.204536\n",
      "[2, 9150] train loss: 4.323242\n",
      "[2, 9150] val loss: 4.347010\n",
      "[2, 9300] train loss: 4.375772\n",
      "[2, 9300] val loss: 4.475039\n",
      "[2, 9450] train loss: 4.344154\n",
      "[2, 9450] val loss: 4.319634\n",
      "[2, 9600] train loss: 4.310759\n",
      "[2, 9600] val loss: 4.301586\n",
      "[2, 9750] train loss: 4.390019\n",
      "[2, 9750] val loss: 4.192969\n",
      "[2, 9900] train loss: 4.345625\n",
      "[2, 9900] val loss: 4.683879\n",
      "[2,10050] train loss: 4.320491\n",
      "[2,10050] val loss: 4.282341\n",
      "[2,10200] train loss: 4.310652\n",
      "[2,10200] val loss: 4.315103\n",
      "[2,10350] train loss: 4.322548\n",
      "[2,10350] val loss: 4.422812\n",
      "[2,10500] train loss: 4.312368\n",
      "[2,10500] val loss: 4.409222\n",
      "[2,10650] train loss: 4.324990\n",
      "[2,10650] val loss: 4.338524\n",
      "[2,10800] train loss: 4.346898\n",
      "[2,10800] val loss: 4.272602\n",
      "[2,10950] train loss: 4.360563\n",
      "[2,10950] val loss: 4.172837\n",
      "[2,11100] train loss: 4.302419\n",
      "[2,11100] val loss: 4.344325\n",
      "[2,11250] train loss: 4.342957\n",
      "[2,11250] val loss: 4.561244\n",
      "[2,11400] train loss: 4.286934\n",
      "[2,11400] val loss: 4.239897\n",
      "[2,11550] train loss: 4.303457\n",
      "[2,11550] val loss: 4.280504\n",
      "[2,11700] train loss: 4.331672\n",
      "[2,11700] val loss: 4.346537\n",
      "[2,11850] train loss: 4.328901\n",
      "[2,11850] val loss: 4.262761\n",
      "[2,12000] train loss: 4.320891\n",
      "[2,12000] val loss: 4.317005\n",
      "[2,12150] train loss: 4.293179\n",
      "[2,12150] val loss: 4.209373\n",
      "[2,12300] train loss: 4.315155\n",
      "[2,12300] val loss: 4.199518\n",
      "[2,12450] train loss: 4.297935\n",
      "[2,12450] val loss: 4.386694\n",
      "[2,12600] train loss: 4.296351\n",
      "[2,12600] val loss: 4.156220\n",
      "[2,12750] train loss: 4.286467\n",
      "[2,12750] val loss: 4.378046\n",
      "[2,12900] train loss: 4.288582\n",
      "[2,12900] val loss: 4.320925\n",
      "[2,13050] train loss: 4.311132\n",
      "[2,13050] val loss: 4.250416\n",
      "[2,13200] train loss: 4.296918\n",
      "[2,13200] val loss: 4.419963\n",
      "[2,13350] train loss: 4.270498\n",
      "[2,13350] val loss: 4.375174\n",
      "[2,13500] train loss: 4.303754\n",
      "[2,13500] val loss: 4.270859\n",
      "[2,13650] train loss: 4.260690\n",
      "[2,13650] val loss: 4.228861\n",
      "[2,13800] train loss: 4.299583\n",
      "[2,13800] val loss: 4.339030\n",
      "[2,13950] train loss: 4.278646\n",
      "[2,13950] val loss: 4.124935\n",
      "[2,14100] train loss: 4.309332\n",
      "[2,14100] val loss: 4.391877\n",
      "[2,14250] train loss: 4.270184\n",
      "[2,14250] val loss: 4.081446\n",
      "[2,14400] train loss: 4.316284\n",
      "[2,14400] val loss: 4.271563\n",
      "[2,14550] train loss: 4.264091\n",
      "[2,14550] val loss: 3.925364\n",
      "[2,14700] train loss: 4.290901\n",
      "[2,14700] val loss: 4.392889\n",
      "[2,14850] train loss: 4.268819\n",
      "[2,14850] val loss: 4.353136\n",
      "[2,15000] train loss: 4.294443\n",
      "[2,15000] val loss: 4.455770\n",
      "[2,15150] train loss: 4.258856\n",
      "[2,15150] val loss: 4.139877\n",
      "[2,15300] train loss: 4.258350\n",
      "[2,15300] val loss: 4.489520\n",
      "[2,15450] train loss: 4.247614\n",
      "[2,15450] val loss: 4.206779\n",
      "[2,15600] train loss: 4.278441\n",
      "[2,15600] val loss: 4.343133\n",
      "[2,15750] train loss: 4.278488\n",
      "[2,15750] val loss: 4.118826\n",
      "[2,15900] train loss: 4.272755\n",
      "[2,15900] val loss: 4.076446\n",
      "[2,16050] train loss: 4.248112\n",
      "[2,16050] val loss: 4.455260\n",
      "[2,16200] train loss: 4.242172\n",
      "[2,16200] val loss: 4.156029\n",
      "[2,16350] train loss: 4.269442\n",
      "[2,16350] val loss: 4.213845\n",
      "[2,16500] train loss: 4.240119\n",
      "[2,16500] val loss: 4.403138\n",
      "[2,16650] train loss: 4.253071\n",
      "[2,16650] val loss: 4.278900\n",
      "[2,16800] train loss: 4.244700\n",
      "[2,16800] val loss: 4.257318\n",
      "[2,16950] train loss: 4.244211\n",
      "[2,16950] val loss: 4.339600\n",
      "[2,17100] train loss: 4.240853\n",
      "[2,17100] val loss: 4.309794\n",
      "[2,17250] train loss: 4.239119\n",
      "[2,17250] val loss: 4.261610\n",
      "[2,17400] train loss: 4.225428\n",
      "[2,17400] val loss: 4.225349\n",
      "[2,17550] train loss: 4.262764\n",
      "[2,17550] val loss: 4.266854\n",
      "[2,17700] train loss: 4.241269\n",
      "[2,17700] val loss: 4.423878\n",
      "[2,17850] train loss: 4.230667\n",
      "[2,17850] val loss: 4.053173\n",
      "[2,18000] train loss: 4.238003\n",
      "[2,18000] val loss: 4.347668\n",
      "[2,18150] train loss: 4.243988\n",
      "[2,18150] val loss: 4.088710\n",
      "[2,18300] train loss: 4.230994\n",
      "[2,18300] val loss: 4.201064\n",
      "[2,18450] train loss: 4.245879\n",
      "[2,18450] val loss: 4.333925\n",
      "[2,18600] train loss: 4.221833\n",
      "[2,18600] val loss: 3.840751\n",
      "[2,18750] train loss: 4.205420\n",
      "[2,18750] val loss: 4.199205\n",
      "[2,18900] train loss: 4.197378\n",
      "[2,18900] val loss: 4.380569\n",
      "[2,19050] train loss: 4.229342\n",
      "[2,19050] val loss: 4.245248\n",
      "[2,19200] train loss: 4.207323\n",
      "[2,19200] val loss: 4.187706\n",
      "[3,  150] train loss: 4.185789\n",
      "[3,  150] val loss: 4.176715\n",
      "[3,  300] train loss: 4.170717\n",
      "[3,  300] val loss: 4.133870\n",
      "[3,  450] train loss: 4.155153\n",
      "[3,  450] val loss: 4.294905\n",
      "[3,  600] train loss: 4.129105\n",
      "[3,  600] val loss: 4.204955\n",
      "[3,  750] train loss: 4.152290\n",
      "[3,  750] val loss: 4.277191\n",
      "[3,  900] train loss: 4.175445\n",
      "[3,  900] val loss: 4.268917\n",
      "[3, 1050] train loss: 4.168948\n",
      "[3, 1050] val loss: 4.244876\n",
      "[3, 1200] train loss: 4.156344\n",
      "[3, 1200] val loss: 4.162063\n",
      "[3, 1350] train loss: 4.148222\n",
      "[3, 1350] val loss: 4.027135\n",
      "[3, 1500] train loss: 4.164466\n",
      "[3, 1500] val loss: 4.296283\n",
      "[3, 1650] train loss: 4.163846\n",
      "[3, 1650] val loss: 4.095541\n",
      "[3, 1800] train loss: 4.173237\n",
      "[3, 1800] val loss: 4.322419\n",
      "[3, 1950] train loss: 4.156745\n",
      "[3, 1950] val loss: 4.327015\n",
      "[3, 2100] train loss: 4.153673\n",
      "[3, 2100] val loss: 4.129519\n",
      "[3, 2250] train loss: 4.174002\n",
      "[3, 2250] val loss: 4.153674\n",
      "[3, 2400] train loss: 4.156949\n",
      "[3, 2400] val loss: 4.279456\n",
      "[3, 2550] train loss: 4.160320\n",
      "[3, 2550] val loss: 4.270487\n",
      "[3, 2700] train loss: 4.177257\n",
      "[3, 2700] val loss: 4.214277\n",
      "[3, 2850] train loss: 4.147901\n",
      "[3, 2850] val loss: 4.291637\n",
      "[3, 3000] train loss: 4.144667\n",
      "[3, 3000] val loss: 4.475763\n",
      "[3, 3150] train loss: 4.141995\n",
      "[3, 3150] val loss: 4.050687\n",
      "[3, 3300] train loss: 4.186475\n",
      "[3, 3300] val loss: 4.148275\n",
      "[3, 3450] train loss: 4.162502\n",
      "[3, 3450] val loss: 4.125656\n",
      "[3, 3600] train loss: 4.174839\n",
      "[3, 3600] val loss: 4.038710\n",
      "[3, 3750] train loss: 4.138739\n",
      "[3, 3750] val loss: 4.299142\n",
      "[3, 3900] train loss: 4.133705\n",
      "[3, 3900] val loss: 4.114142\n",
      "[3, 4050] train loss: 4.144224\n",
      "[3, 4050] val loss: 4.258506\n",
      "[3, 4200] train loss: 4.165381\n",
      "[3, 4200] val loss: 4.002403\n",
      "[3, 4350] train loss: 4.165848\n",
      "[3, 4350] val loss: 4.251200\n",
      "[3, 4500] train loss: 4.177850\n",
      "[3, 4500] val loss: 4.177800\n",
      "[3, 4650] train loss: 4.139800\n",
      "[3, 4650] val loss: 4.360643\n",
      "[3, 4800] train loss: 4.154276\n",
      "[3, 4800] val loss: 4.146606\n",
      "[3, 4950] train loss: 4.146501\n",
      "[3, 4950] val loss: 4.188799\n",
      "[3, 5100] train loss: 4.124053\n",
      "[3, 5100] val loss: 4.144196\n",
      "[3, 5250] train loss: 4.159054\n",
      "[3, 5250] val loss: 4.195631\n",
      "[3, 5400] train loss: 4.157394\n",
      "[3, 5400] val loss: 4.320462\n",
      "[3, 5550] train loss: 4.106251\n",
      "[3, 5550] val loss: 4.275364\n",
      "[3, 5700] train loss: 4.142009\n",
      "[3, 5700] val loss: 4.209735\n",
      "[3, 5850] train loss: 4.102117\n",
      "[3, 5850] val loss: 4.090115\n",
      "[3, 6000] train loss: 4.151651\n",
      "[3, 6000] val loss: 4.095391\n",
      "[3, 6150] train loss: 4.135031\n",
      "[3, 6150] val loss: 4.039614\n",
      "[3, 6300] train loss: 4.155940\n",
      "[3, 6300] val loss: 4.073430\n",
      "[3, 6450] train loss: 4.121629\n",
      "[3, 6450] val loss: 4.030014\n",
      "[3, 6600] train loss: 4.123990\n",
      "[3, 6600] val loss: 4.107649\n",
      "[3, 6750] train loss: 4.129955\n",
      "[3, 6750] val loss: 4.145198\n",
      "[3, 6900] train loss: 4.108489\n",
      "[3, 6900] val loss: 4.139736\n",
      "[3, 7050] train loss: 4.115004\n",
      "[3, 7050] val loss: 4.068757\n",
      "[3, 7200] train loss: 4.121039\n",
      "[3, 7200] val loss: 3.944729\n",
      "[3, 7350] train loss: 4.147169\n",
      "[3, 7350] val loss: 4.242748\n",
      "[3, 7500] train loss: 4.098376\n",
      "[3, 7500] val loss: 4.167706\n",
      "[3, 7650] train loss: 4.115432\n",
      "[3, 7650] val loss: 4.301975\n",
      "[3, 7800] train loss: 4.134243\n",
      "[3, 7800] val loss: 4.113107\n",
      "[3, 7950] train loss: 4.109450\n",
      "[3, 7950] val loss: 4.096144\n",
      "[3, 8100] train loss: 4.117913\n",
      "[3, 8100] val loss: 4.228172\n",
      "[3, 8250] train loss: 4.138446\n",
      "[3, 8250] val loss: 4.097232\n",
      "[3, 8400] train loss: 4.105398\n",
      "[3, 8400] val loss: 4.285788\n",
      "[3, 8550] train loss: 4.136595\n",
      "[3, 8550] val loss: 4.040444\n",
      "[3, 8700] train loss: 4.110580\n",
      "[3, 8700] val loss: 4.077011\n",
      "[3, 8850] train loss: 4.103228\n",
      "[3, 8850] val loss: 4.216671\n",
      "[3, 9000] train loss: 4.096915\n",
      "[3, 9000] val loss: 4.020883\n",
      "[3, 9150] train loss: 4.125507\n",
      "[3, 9150] val loss: 4.236445\n",
      "[3, 9300] train loss: 4.081794\n",
      "[3, 9300] val loss: 4.058110\n",
      "[3, 9450] train loss: 4.102326\n",
      "[3, 9450] val loss: 4.147831\n",
      "[3, 9600] train loss: 4.102062\n",
      "[3, 9600] val loss: 4.036584\n",
      "[3, 9750] train loss: 4.082156\n",
      "[3, 9750] val loss: 4.088908\n",
      "[3, 9900] train loss: 4.091207\n",
      "[3, 9900] val loss: 3.980999\n",
      "[3,10050] train loss: 4.105723\n",
      "[3,10050] val loss: 4.234187\n",
      "[3,10200] train loss: 4.092947\n",
      "[3,10200] val loss: 4.153833\n",
      "[3,10350] train loss: 4.108119\n",
      "[3,10350] val loss: 4.073289\n",
      "[3,10500] train loss: 4.087338\n",
      "[3,10500] val loss: 4.140752\n",
      "[3,10650] train loss: 4.114618\n",
      "[3,10650] val loss: 4.177987\n",
      "[3,10800] train loss: 4.082351\n",
      "[3,10800] val loss: 4.184428\n",
      "[3,10950] train loss: 4.079842\n",
      "[3,10950] val loss: 4.272300\n",
      "[3,11100] train loss: 4.085589\n",
      "[3,11100] val loss: 4.268516\n",
      "[3,11250] train loss: 4.130499\n",
      "[3,11250] val loss: 4.300731\n",
      "[3,11400] train loss: 4.090119\n",
      "[3,11400] val loss: 4.105727\n",
      "[3,11550] train loss: 4.110132\n",
      "[3,11550] val loss: 4.041254\n",
      "[3,11700] train loss: 4.092106\n",
      "[3,11700] val loss: 3.999565\n",
      "[3,11850] train loss: 4.097852\n",
      "[3,11850] val loss: 4.464262\n",
      "[3,12000] train loss: 4.121101\n",
      "[3,12000] val loss: 4.093810\n",
      "[3,12150] train loss: 4.056950\n",
      "[3,12150] val loss: 4.066979\n",
      "[3,12300] train loss: 4.100201\n",
      "[3,12300] val loss: 4.281504\n",
      "[3,12450] train loss: 4.061228\n",
      "[3,12450] val loss: 4.122681\n",
      "[3,12600] train loss: 4.091417\n",
      "[3,12600] val loss: 4.188165\n",
      "[3,12750] train loss: 4.071851\n",
      "[3,12750] val loss: 3.913202\n",
      "[3,12900] train loss: 4.049729\n",
      "[3,12900] val loss: 3.934684\n",
      "[3,13050] train loss: 4.071692\n",
      "[3,13050] val loss: 3.915032\n",
      "[3,13200] train loss: 4.069131\n",
      "[3,13200] val loss: 4.241772\n",
      "[3,13350] train loss: 4.115333\n",
      "[3,13350] val loss: 4.035206\n",
      "[3,13500] train loss: 4.071840\n",
      "[3,13500] val loss: 4.065507\n",
      "[3,13650] train loss: 4.056569\n",
      "[3,13650] val loss: 3.849551\n",
      "[3,13800] train loss: 4.062114\n",
      "[3,13800] val loss: 4.007155\n",
      "[3,13950] train loss: 4.074712\n",
      "[3,13950] val loss: 4.008778\n",
      "[3,14100] train loss: 4.094995\n",
      "[3,14100] val loss: 4.279406\n",
      "[3,14250] train loss: 4.069440\n",
      "[3,14250] val loss: 4.114592\n",
      "[3,14400] train loss: 4.064678\n",
      "[3,14400] val loss: 4.033742\n",
      "[3,14550] train loss: 4.094147\n",
      "[3,14550] val loss: 4.153268\n",
      "[3,14700] train loss: 4.065044\n",
      "[3,14700] val loss: 3.959588\n",
      "[3,14850] train loss: 4.071012\n",
      "[3,14850] val loss: 4.058897\n",
      "[3,15000] train loss: 4.050561\n",
      "[3,15000] val loss: 4.176655\n",
      "[3,15150] train loss: 4.061282\n",
      "[3,15150] val loss: 4.067838\n",
      "[3,15300] train loss: 4.067300\n",
      "[3,15300] val loss: 4.044725\n",
      "[3,15450] train loss: 4.074432\n",
      "[3,15450] val loss: 4.074297\n",
      "[3,15600] train loss: 4.079790\n",
      "[3,15600] val loss: 4.175677\n",
      "[3,15750] train loss: 4.060316\n",
      "[3,15750] val loss: 4.028617\n",
      "[3,15900] train loss: 4.087174\n",
      "[3,15900] val loss: 4.023242\n",
      "[3,16050] train loss: 4.065117\n",
      "[3,16050] val loss: 4.079725\n",
      "[3,16200] train loss: 4.036455\n",
      "[3,16200] val loss: 4.133208\n",
      "[3,16350] train loss: 4.056030\n",
      "[3,16350] val loss: 4.011492\n",
      "[3,16500] train loss: 4.056012\n",
      "[3,16500] val loss: 4.159121\n",
      "[3,16650] train loss: 4.098911\n",
      "[3,16650] val loss: 4.130897\n",
      "[3,16800] train loss: 4.050546\n",
      "[3,16800] val loss: 4.082870\n",
      "[3,16950] train loss: 4.048745\n",
      "[3,16950] val loss: 3.885950\n",
      "[3,17100] train loss: 4.079062\n",
      "[3,17100] val loss: 3.923121\n",
      "[3,17250] train loss: 4.047608\n",
      "[3,17250] val loss: 4.155782\n",
      "[3,17400] train loss: 4.071044\n",
      "[3,17400] val loss: 4.159403\n",
      "[3,17550] train loss: 4.020654\n",
      "[3,17550] val loss: 3.957078\n",
      "[3,17700] train loss: 4.045410\n",
      "[3,17700] val loss: 4.195762\n",
      "[3,17850] train loss: 4.008776\n",
      "[3,17850] val loss: 4.147059\n",
      "[3,18000] train loss: 4.035809\n",
      "[3,18000] val loss: 4.084258\n",
      "[3,18150] train loss: 4.057621\n",
      "[3,18150] val loss: 4.165420\n",
      "[3,18300] train loss: 4.045485\n",
      "[3,18300] val loss: 4.148200\n",
      "[3,18450] train loss: 4.054322\n",
      "[3,18450] val loss: 3.957815\n",
      "[3,18600] train loss: 4.031854\n",
      "[3,18600] val loss: 4.073792\n",
      "[3,18750] train loss: 4.022252\n",
      "[3,18750] val loss: 4.323140\n",
      "[3,18900] train loss: 4.028136\n",
      "[3,18900] val loss: 3.988724\n",
      "[3,19050] train loss: 4.028770\n",
      "[3,19050] val loss: 4.074061\n",
      "[3,19200] train loss: 4.044613\n",
      "[3,19200] val loss: 3.865923\n",
      "[4,  150] train loss: 4.014891\n",
      "[4,  150] val loss: 4.046517\n",
      "[4,  300] train loss: 4.014127\n",
      "[4,  300] val loss: 4.112497\n",
      "[4,  450] train loss: 3.989336\n",
      "[4,  450] val loss: 3.878995\n",
      "[4,  600] train loss: 3.971531\n",
      "[4,  600] val loss: 4.206231\n",
      "[4,  750] train loss: 4.004353\n",
      "[4,  750] val loss: 3.881317\n",
      "[4,  900] train loss: 3.999187\n",
      "[4,  900] val loss: 3.926287\n",
      "[4, 1050] train loss: 3.989838\n",
      "[4, 1050] val loss: 4.234802\n",
      "[4, 1200] train loss: 3.988186\n",
      "[4, 1200] val loss: 4.156590\n",
      "[4, 1350] train loss: 3.993015\n",
      "[4, 1350] val loss: 3.924769\n",
      "[4, 1500] train loss: 3.947426\n",
      "[4, 1500] val loss: 4.165569\n",
      "[4, 1650] train loss: 3.993892\n",
      "[4, 1650] val loss: 3.956652\n",
      "[4, 1800] train loss: 3.989229\n",
      "[4, 1800] val loss: 3.936922\n",
      "[4, 1950] train loss: 3.983656\n",
      "[4, 1950] val loss: 4.127057\n",
      "[4, 2100] train loss: 3.974671\n",
      "[4, 2100] val loss: 4.150118\n",
      "[4, 2250] train loss: 3.980121\n",
      "[4, 2250] val loss: 4.135182\n",
      "[4, 2400] train loss: 3.993261\n",
      "[4, 2400] val loss: 4.173245\n",
      "[4, 2550] train loss: 3.962206\n",
      "[4, 2550] val loss: 4.159135\n",
      "[4, 2700] train loss: 3.963046\n",
      "[4, 2700] val loss: 3.896762\n",
      "[4, 2850] train loss: 3.994659\n",
      "[4, 2850] val loss: 4.222455\n",
      "[4, 3000] train loss: 3.982679\n",
      "[4, 3000] val loss: 4.091926\n",
      "[4, 3150] train loss: 4.000631\n",
      "[4, 3150] val loss: 4.215967\n",
      "[4, 3300] train loss: 3.989192\n",
      "[4, 3300] val loss: 4.291576\n",
      "[4, 3450] train loss: 4.003317\n",
      "[4, 3450] val loss: 4.111208\n",
      "[4, 3600] train loss: 4.012489\n",
      "[4, 3600] val loss: 4.028800\n",
      "[4, 3750] train loss: 4.000026\n",
      "[4, 3750] val loss: 4.066161\n",
      "[4, 3900] train loss: 3.987466\n",
      "[4, 3900] val loss: 4.061120\n",
      "[4, 4050] train loss: 3.989190\n",
      "[4, 4050] val loss: 4.025694\n",
      "[4, 4200] train loss: 3.955750\n",
      "[4, 4200] val loss: 3.923207\n",
      "[4, 4350] train loss: 3.985427\n",
      "[4, 4350] val loss: 3.957500\n",
      "[4, 4500] train loss: 3.982456\n",
      "[4, 4500] val loss: 4.269254\n",
      "[4, 4650] train loss: 3.996969\n",
      "[4, 4650] val loss: 4.181882\n",
      "[4, 4800] train loss: 3.938259\n",
      "[4, 4800] val loss: 4.108585\n",
      "[4, 4950] train loss: 3.994196\n",
      "[4, 4950] val loss: 3.939745\n",
      "[4, 5100] train loss: 3.983046\n",
      "[4, 5100] val loss: 4.139616\n",
      "[4, 5250] train loss: 3.957962\n",
      "[4, 5250] val loss: 4.163677\n",
      "[4, 5400] train loss: 3.984050\n",
      "[4, 5400] val loss: 3.829286\n",
      "[4, 5550] train loss: 3.946324\n",
      "[4, 5550] val loss: 3.935088\n",
      "[4, 5700] train loss: 3.968024\n",
      "[4, 5700] val loss: 4.069880\n",
      "[4, 5850] train loss: 3.962462\n",
      "[4, 5850] val loss: 4.073843\n",
      "[4, 6000] train loss: 3.984301\n",
      "[4, 6000] val loss: 3.875220\n",
      "[4, 6150] train loss: 4.001434\n",
      "[4, 6150] val loss: 4.233552\n",
      "[4, 6300] train loss: 3.982199\n",
      "[4, 6300] val loss: 3.891931\n",
      "[4, 6450] train loss: 3.974160\n",
      "[4, 6450] val loss: 4.056023\n",
      "[4, 6600] train loss: 3.972533\n",
      "[4, 6600] val loss: 3.848837\n",
      "[4, 6750] train loss: 3.998558\n",
      "[4, 6750] val loss: 3.875081\n",
      "[4, 6900] train loss: 3.957375\n",
      "[4, 6900] val loss: 3.986836\n",
      "[4, 7050] train loss: 3.963464\n",
      "[4, 7050] val loss: 3.875280\n",
      "[4, 7200] train loss: 4.006844\n",
      "[4, 7200] val loss: 4.211625\n",
      "[4, 7350] train loss: 3.961411\n",
      "[4, 7350] val loss: 4.003961\n",
      "[4, 7500] train loss: 3.981087\n",
      "[4, 7500] val loss: 3.893866\n",
      "[4, 7650] train loss: 3.939198\n",
      "[4, 7650] val loss: 4.011950\n",
      "[4, 7800] train loss: 3.977218\n",
      "[4, 7800] val loss: 4.011015\n",
      "[4, 7950] train loss: 3.964891\n",
      "[4, 7950] val loss: 3.979359\n",
      "[4, 8100] train loss: 3.984457\n",
      "[4, 8100] val loss: 3.904494\n",
      "[4, 8250] train loss: 3.950634\n",
      "[4, 8250] val loss: 3.956701\n",
      "[4, 8400] train loss: 3.980625\n",
      "[4, 8400] val loss: 4.021965\n",
      "[4, 8550] train loss: 3.985732\n",
      "[4, 8550] val loss: 3.982797\n",
      "[4, 8700] train loss: 3.981540\n",
      "[4, 8700] val loss: 3.807601\n",
      "[4, 8850] train loss: 3.974609\n",
      "[4, 8850] val loss: 4.054505\n",
      "[4, 9000] train loss: 3.975736\n",
      "[4, 9000] val loss: 3.904720\n",
      "[4, 9150] train loss: 3.950376\n",
      "[4, 9150] val loss: 3.769604\n",
      "[4, 9300] train loss: 3.972528\n",
      "[4, 9300] val loss: 4.079203\n",
      "[4, 9450] train loss: 3.967635\n",
      "[4, 9450] val loss: 4.096789\n",
      "[4, 9600] train loss: 3.961410\n",
      "[4, 9600] val loss: 4.095022\n",
      "[4, 9750] train loss: 3.956243\n",
      "[4, 9750] val loss: 4.028452\n",
      "[4, 9900] train loss: 3.973460\n",
      "[4, 9900] val loss: 4.028908\n",
      "[4,10050] train loss: 3.907128\n",
      "[4,10050] val loss: 3.758350\n",
      "[4,10200] train loss: 3.955873\n",
      "[4,10200] val loss: 4.155304\n",
      "[4,10350] train loss: 3.943728\n",
      "[4,10350] val loss: 3.901652\n",
      "[4,10500] train loss: 3.994972\n",
      "[4,10500] val loss: 3.966797\n",
      "[4,10650] train loss: 3.958190\n",
      "[4,10650] val loss: 4.086178\n",
      "[4,10800] train loss: 3.939248\n",
      "[4,10800] val loss: 4.097381\n",
      "[4,10950] train loss: 3.981417\n",
      "[4,10950] val loss: 4.130067\n",
      "[4,11100] train loss: 3.936315\n",
      "[4,11100] val loss: 4.003058\n",
      "[4,11250] train loss: 3.970850\n",
      "[4,11250] val loss: 3.917743\n",
      "[4,11400] train loss: 3.966619\n",
      "[4,11400] val loss: 3.950459\n",
      "[4,11550] train loss: 3.965365\n",
      "[4,11550] val loss: 3.927422\n",
      "[4,11700] train loss: 3.976923\n",
      "[4,11700] val loss: 3.850140\n",
      "[4,11850] train loss: 3.984238\n",
      "[4,11850] val loss: 4.003327\n",
      "[4,12000] train loss: 3.958892\n",
      "[4,12000] val loss: 3.935966\n",
      "[4,12150] train loss: 3.939672\n",
      "[4,12150] val loss: 3.919138\n",
      "[4,12300] train loss: 3.959587\n",
      "[4,12300] val loss: 3.921918\n",
      "[4,12450] train loss: 3.943807\n",
      "[4,12450] val loss: 3.851783\n",
      "[4,12600] train loss: 3.932399\n",
      "[4,12600] val loss: 4.254347\n",
      "[4,12750] train loss: 3.927305\n",
      "[4,12750] val loss: 4.113872\n",
      "[4,12900] train loss: 3.969113\n",
      "[4,12900] val loss: 3.908850\n",
      "[4,13050] train loss: 3.952284\n",
      "[4,13050] val loss: 3.931960\n",
      "[4,13200] train loss: 3.951538\n",
      "[4,13200] val loss: 3.970516\n",
      "[4,13350] train loss: 3.956643\n",
      "[4,13350] val loss: 4.077911\n",
      "[4,13500] train loss: 3.923631\n",
      "[4,13500] val loss: 3.804857\n",
      "[4,13650] train loss: 3.949070\n",
      "[4,13650] val loss: 4.036165\n",
      "[4,13800] train loss: 3.939790\n",
      "[4,13800] val loss: 3.970122\n",
      "[4,13950] train loss: 3.932692\n",
      "[4,13950] val loss: 3.999698\n",
      "[4,14100] train loss: 3.915095\n",
      "[4,14100] val loss: 3.830976\n",
      "[4,14250] train loss: 3.928424\n",
      "[4,14250] val loss: 4.025411\n",
      "[4,14400] train loss: 3.957263\n",
      "[4,14400] val loss: 4.104480\n",
      "[4,14550] train loss: 3.978229\n",
      "[4,14550] val loss: 3.858451\n",
      "[4,14700] train loss: 3.921122\n",
      "[4,14700] val loss: 3.910391\n",
      "[4,14850] train loss: 3.961009\n",
      "[4,14850] val loss: 4.008769\n",
      "[4,15000] train loss: 3.941531\n",
      "[4,15000] val loss: 4.022904\n",
      "[4,15150] train loss: 3.912182\n",
      "[4,15150] val loss: 3.899140\n",
      "[4,15300] train loss: 3.951590\n",
      "[4,15300] val loss: 3.897866\n",
      "[4,15450] train loss: 3.959971\n",
      "[4,15450] val loss: 4.049225\n",
      "[4,15600] train loss: 3.972692\n",
      "[4,15600] val loss: 4.025288\n",
      "[4,15750] train loss: 3.945553\n",
      "[4,15750] val loss: 3.912064\n",
      "[4,15900] train loss: 3.939867\n",
      "[4,15900] val loss: 4.074955\n",
      "[4,16050] train loss: 3.941198\n",
      "[4,16050] val loss: 3.895480\n",
      "[4,16200] train loss: 3.921339\n",
      "[4,16200] val loss: 3.942117\n",
      "[4,16350] train loss: 3.911412\n",
      "[4,16350] val loss: 3.905302\n",
      "[4,16500] train loss: 3.916776\n",
      "[4,16500] val loss: 3.921773\n",
      "[4,16650] train loss: 3.908873\n",
      "[4,16650] val loss: 4.045012\n",
      "[4,16800] train loss: 3.914781\n",
      "[4,16800] val loss: 4.119999\n",
      "[4,16950] train loss: 3.954438\n",
      "[4,16950] val loss: 4.027084\n",
      "[4,17100] train loss: 3.911706\n",
      "[4,17100] val loss: 3.912384\n",
      "[4,17250] train loss: 3.898147\n",
      "[4,17250] val loss: 3.985274\n",
      "[4,17400] train loss: 3.954624\n",
      "[4,17400] val loss: 3.765878\n",
      "[4,17550] train loss: 3.952566\n",
      "[4,17550] val loss: 3.956992\n",
      "[4,17700] train loss: 3.914902\n",
      "[4,17700] val loss: 3.757968\n",
      "[4,17850] train loss: 3.918719\n",
      "[4,17850] val loss: 4.089556\n",
      "[4,18000] train loss: 3.951972\n",
      "[4,18000] val loss: 3.917468\n",
      "[4,18150] train loss: 3.961202\n",
      "[4,18150] val loss: 3.951142\n",
      "[4,18300] train loss: 3.911459\n",
      "[4,18300] val loss: 4.057602\n",
      "[4,18450] train loss: 3.924178\n",
      "[4,18450] val loss: 3.920722\n",
      "[4,18600] train loss: 3.933529\n",
      "[4,18600] val loss: 4.008864\n",
      "[4,18750] train loss: 3.933073\n",
      "[4,18750] val loss: 4.130180\n",
      "[4,18900] train loss: 3.917376\n",
      "[4,18900] val loss: 4.000218\n",
      "[4,19050] train loss: 3.938122\n",
      "[4,19050] val loss: 3.918075\n",
      "[4,19200] train loss: 3.940335\n",
      "[4,19200] val loss: 3.867637\n",
      "[5,  150] train loss: 3.867020\n",
      "[5,  150] val loss: 3.828771\n",
      "[5,  300] train loss: 3.853972\n",
      "[5,  300] val loss: 3.968275\n",
      "[5,  450] train loss: 3.873509\n",
      "[5,  450] val loss: 3.768077\n",
      "[5,  600] train loss: 3.885628\n",
      "[5,  600] val loss: 3.874077\n",
      "[5,  750] train loss: 3.864368\n",
      "[5,  750] val loss: 3.948567\n",
      "[5,  900] train loss: 3.869913\n",
      "[5,  900] val loss: 3.964027\n",
      "[5, 1050] train loss: 3.899986\n",
      "[5, 1050] val loss: 3.798911\n",
      "[5, 1200] train loss: 3.895157\n",
      "[5, 1200] val loss: 3.973298\n",
      "[5, 1350] train loss: 3.892546\n",
      "[5, 1350] val loss: 3.793259\n",
      "[5, 1500] train loss: 3.866206\n",
      "[5, 1500] val loss: 3.823159\n",
      "[5, 1650] train loss: 3.882907\n",
      "[5, 1650] val loss: 4.099575\n",
      "[5, 1800] train loss: 3.878379\n",
      "[5, 1800] val loss: 3.870288\n",
      "[5, 1950] train loss: 3.896464\n",
      "[5, 1950] val loss: 3.872070\n",
      "[5, 2100] train loss: 3.905039\n",
      "[5, 2100] val loss: 3.981711\n",
      "[5, 2250] train loss: 3.888534\n",
      "[5, 2250] val loss: 3.851469\n",
      "[5, 2400] train loss: 3.898040\n",
      "[5, 2400] val loss: 3.757945\n",
      "[5, 2550] train loss: 3.875093\n",
      "[5, 2550] val loss: 3.785520\n",
      "[5, 2700] train loss: 3.885388\n",
      "[5, 2700] val loss: 3.805062\n",
      "[5, 2850] train loss: 3.893973\n",
      "[5, 2850] val loss: 3.980231\n",
      "[5, 3000] train loss: 3.860503\n",
      "[5, 3000] val loss: 3.813183\n",
      "[5, 3150] train loss: 3.893559\n",
      "[5, 3150] val loss: 3.745095\n",
      "[5, 3300] train loss: 3.897837\n",
      "[5, 3300] val loss: 3.743398\n",
      "[5, 3450] train loss: 3.871645\n",
      "[5, 3450] val loss: 4.036412\n",
      "[5, 3600] train loss: 3.872402\n",
      "[5, 3600] val loss: 3.750011\n",
      "[5, 3750] train loss: 3.906291\n",
      "[5, 3750] val loss: 3.799569\n",
      "[5, 3900] train loss: 3.854928\n",
      "[5, 3900] val loss: 3.680496\n",
      "[5, 4050] train loss: 3.833389\n",
      "[5, 4050] val loss: 3.957286\n",
      "[5, 4200] train loss: 3.883146\n",
      "[5, 4200] val loss: 3.665494\n",
      "[5, 4350] train loss: 3.866987\n",
      "[5, 4350] val loss: 3.967956\n",
      "[5, 4500] train loss: 3.843056\n",
      "[5, 4500] val loss: 4.101129\n",
      "[5, 4650] train loss: 3.865668\n",
      "[5, 4650] val loss: 3.994298\n",
      "[5, 4800] train loss: 3.864822\n",
      "[5, 4800] val loss: 3.923866\n",
      "[5, 4950] train loss: 3.904033\n",
      "[5, 4950] val loss: 3.915733\n",
      "[5, 5100] train loss: 3.898040\n",
      "[5, 5100] val loss: 4.154299\n",
      "[5, 5250] train loss: 3.893353\n",
      "[5, 5250] val loss: 3.999277\n",
      "[5, 5400] train loss: 3.848023\n",
      "[5, 5400] val loss: 4.038348\n",
      "[5, 5550] train loss: 3.870809\n",
      "[5, 5550] val loss: 3.900850\n",
      "[5, 5700] train loss: 3.865568\n",
      "[5, 5700] val loss: 4.015015\n",
      "[5, 5850] train loss: 3.870926\n",
      "[5, 5850] val loss: 3.791960\n",
      "[5, 6000] train loss: 3.861044\n",
      "[5, 6000] val loss: 4.028689\n",
      "[5, 6150] train loss: 3.881826\n",
      "[5, 6150] val loss: 4.208341\n",
      "[5, 6300] train loss: 3.888039\n",
      "[5, 6300] val loss: 4.026280\n",
      "[5, 6450] train loss: 3.870153\n",
      "[5, 6450] val loss: 3.960812\n",
      "[5, 6600] train loss: 3.891938\n",
      "[5, 6600] val loss: 3.927865\n",
      "[5, 6750] train loss: 3.878004\n",
      "[5, 6750] val loss: 3.968912\n",
      "[5, 6900] train loss: 3.860595\n",
      "[5, 6900] val loss: 4.084303\n",
      "[5, 7050] train loss: 3.883454\n",
      "[5, 7050] val loss: 3.954416\n",
      "[5, 7200] train loss: 3.895915\n",
      "[5, 7200] val loss: 3.914127\n",
      "[5, 7350] train loss: 3.865389\n",
      "[5, 7350] val loss: 4.035388\n",
      "[5, 7500] train loss: 3.902374\n",
      "[5, 7500] val loss: 3.921398\n",
      "[5, 7650] train loss: 3.870476\n",
      "[5, 7650] val loss: 3.959594\n",
      "[5, 7800] train loss: 3.857292\n",
      "[5, 7800] val loss: 4.046762\n",
      "[5, 7950] train loss: 3.866990\n",
      "[5, 7950] val loss: 3.808587\n",
      "[5, 8100] train loss: 3.882732\n",
      "[5, 8100] val loss: 3.851188\n",
      "[5, 8250] train loss: 3.863131\n",
      "[5, 8250] val loss: 3.889245\n",
      "[5, 8400] train loss: 3.860597\n",
      "[5, 8400] val loss: 4.074371\n",
      "[5, 8550] train loss: 3.857046\n",
      "[5, 8550] val loss: 3.892636\n",
      "[5, 8700] train loss: 3.865866\n",
      "[5, 8700] val loss: 3.884538\n",
      "[5, 8850] train loss: 3.909966\n",
      "[5, 8850] val loss: 3.796137\n",
      "[5, 9000] train loss: 3.902330\n",
      "[5, 9000] val loss: 3.671233\n",
      "[5, 9150] train loss: 3.847429\n",
      "[5, 9150] val loss: 4.009503\n",
      "[5, 9300] train loss: 3.881635\n",
      "[5, 9300] val loss: 3.806802\n",
      "[5, 9450] train loss: 3.863773\n",
      "[5, 9450] val loss: 3.883918\n",
      "[5, 9600] train loss: 3.877280\n",
      "[5, 9600] val loss: 3.920441\n",
      "[5, 9750] train loss: 3.855404\n",
      "[5, 9750] val loss: 3.856384\n",
      "[5, 9900] train loss: 3.838172\n",
      "[5, 9900] val loss: 3.862249\n",
      "[5,10050] train loss: 3.856437\n",
      "[5,10050] val loss: 4.045718\n",
      "[5,10200] train loss: 3.851185\n",
      "[5,10200] val loss: 3.781340\n",
      "[5,10350] train loss: 3.849108\n",
      "[5,10350] val loss: 3.981406\n",
      "[5,10500] train loss: 3.886983\n",
      "[5,10500] val loss: 4.221331\n",
      "[5,10650] train loss: 3.832311\n",
      "[5,10650] val loss: 3.872985\n",
      "[5,10800] train loss: 3.873316\n",
      "[5,10800] val loss: 3.879273\n",
      "[5,10950] train loss: 3.894513\n",
      "[5,10950] val loss: 4.247948\n",
      "[5,11100] train loss: 3.856028\n",
      "[5,11100] val loss: 3.812044\n",
      "[5,11250] train loss: 3.860331\n",
      "[5,11250] val loss: 3.719839\n",
      "[5,11400] train loss: 3.870324\n",
      "[5,11400] val loss: 3.762846\n",
      "[5,11550] train loss: 3.844955\n",
      "[5,11550] val loss: 3.925752\n",
      "[5,11700] train loss: 3.860957\n",
      "[5,11700] val loss: 3.687279\n",
      "[5,11850] train loss: 3.863046\n",
      "[5,11850] val loss: 3.981794\n",
      "[5,12000] train loss: 3.831266\n",
      "[5,12000] val loss: 3.915113\n",
      "[5,12150] train loss: 3.892554\n",
      "[5,12150] val loss: 3.724564\n",
      "[5,12300] train loss: 3.855556\n",
      "[5,12300] val loss: 3.903923\n",
      "[5,12450] train loss: 3.882084\n",
      "[5,12450] val loss: 3.790721\n",
      "[5,12600] train loss: 3.882595\n",
      "[5,12600] val loss: 3.825943\n",
      "[5,12750] train loss: 3.864010\n",
      "[5,12750] val loss: 4.049927\n",
      "[5,12900] train loss: 3.846260\n",
      "[5,12900] val loss: 3.799233\n",
      "[5,13050] train loss: 3.843299\n",
      "[5,13050] val loss: 3.985482\n",
      "[5,13200] train loss: 3.825554\n",
      "[5,13200] val loss: 3.922202\n",
      "[5,13350] train loss: 3.845859\n",
      "[5,13350] val loss: 3.766075\n",
      "[5,13500] train loss: 3.864732\n",
      "[5,13500] val loss: 3.787778\n",
      "[5,13650] train loss: 3.859339\n",
      "[5,13650] val loss: 3.874232\n",
      "[5,13800] train loss: 3.837499\n",
      "[5,13800] val loss: 3.804604\n",
      "[5,13950] train loss: 3.856664\n",
      "[5,13950] val loss: 3.995543\n",
      "[5,14100] train loss: 3.858657\n",
      "[5,14100] val loss: 3.892614\n",
      "[5,14250] train loss: 3.829052\n",
      "[5,14250] val loss: 3.948609\n",
      "[5,14400] train loss: 3.824828\n",
      "[5,14400] val loss: 3.824454\n",
      "[5,14550] train loss: 3.883956\n",
      "[5,14550] val loss: 3.854626\n",
      "[5,14700] train loss: 3.880941\n",
      "[5,14700] val loss: 3.836834\n",
      "[5,14850] train loss: 3.847323\n",
      "[5,14850] val loss: 3.820957\n",
      "[5,15000] train loss: 3.868185\n",
      "[5,15000] val loss: 3.808214\n",
      "[5,15150] train loss: 3.849370\n",
      "[5,15150] val loss: 3.942931\n",
      "[5,15300] train loss: 3.882066\n",
      "[5,15300] val loss: 3.823208\n",
      "[5,15450] train loss: 3.876116\n",
      "[5,15450] val loss: 3.943034\n",
      "[5,15600] train loss: 3.854957\n",
      "[5,15600] val loss: 3.815608\n",
      "[5,15750] train loss: 3.879050\n",
      "[5,15750] val loss: 3.625439\n",
      "[5,15900] train loss: 3.873017\n",
      "[5,15900] val loss: 3.963528\n",
      "[5,16050] train loss: 3.846599\n",
      "[5,16050] val loss: 3.805044\n",
      "[5,16200] train loss: 3.861013\n",
      "[5,16200] val loss: 3.723497\n",
      "[5,16350] train loss: 3.881494\n",
      "[5,16350] val loss: 3.845209\n",
      "[5,16500] train loss: 3.839406\n",
      "[5,16500] val loss: 3.636225\n",
      "[5,16650] train loss: 3.849692\n",
      "[5,16650] val loss: 3.909184\n",
      "[5,16800] train loss: 3.867640\n",
      "[5,16800] val loss: 3.953438\n",
      "[5,16950] train loss: 3.846230\n",
      "[5,16950] val loss: 3.738760\n",
      "[5,17100] train loss: 3.884319\n",
      "[5,17100] val loss: 3.934680\n",
      "[5,17250] train loss: 3.847666\n",
      "[5,17250] val loss: 4.196609\n",
      "[5,17400] train loss: 3.841606\n",
      "[5,17400] val loss: 3.969854\n",
      "[5,17550] train loss: 3.862374\n",
      "[5,17550] val loss: 3.973574\n",
      "[5,17700] train loss: 3.843875\n",
      "[5,17700] val loss: 3.923828\n",
      "[5,17850] train loss: 3.856229\n",
      "[5,17850] val loss: 3.940811\n",
      "[5,18000] train loss: 3.834457\n",
      "[5,18000] val loss: 3.752305\n",
      "[5,18150] train loss: 3.857723\n",
      "[5,18150] val loss: 4.189804\n",
      "[5,18300] train loss: 3.845032\n",
      "[5,18300] val loss: 3.877913\n",
      "[5,18450] train loss: 3.882723\n",
      "[5,18450] val loss: 3.824844\n",
      "[5,18600] train loss: 3.851125\n",
      "[5,18600] val loss: 3.801638\n",
      "[5,18750] train loss: 3.859601\n",
      "[5,18750] val loss: 3.856582\n",
      "[5,18900] train loss: 3.822793\n",
      "[5,18900] val loss: 3.818206\n",
      "[5,19050] train loss: 3.851989\n",
      "[5,19050] val loss: 3.804578\n",
      "[5,19200] train loss: 3.862036\n",
      "[5,19200] val loss: 3.972148\n",
      "[6,  150] train loss: 3.819794\n",
      "[6,  150] val loss: 3.901114\n",
      "[6,  300] train loss: 3.817567\n",
      "[6,  300] val loss: 3.879238\n",
      "[6,  450] train loss: 3.822887\n",
      "[6,  450] val loss: 3.646877\n",
      "[6,  600] train loss: 3.790119\n",
      "[6,  600] val loss: 4.106959\n",
      "[6,  750] train loss: 3.785207\n",
      "[6,  750] val loss: 3.817511\n",
      "[6,  900] train loss: 3.818805\n",
      "[6,  900] val loss: 3.973770\n",
      "[6, 1050] train loss: 3.811875\n",
      "[6, 1050] val loss: 4.047881\n",
      "[6, 1200] train loss: 3.817867\n",
      "[6, 1200] val loss: 3.967053\n",
      "[6, 1350] train loss: 3.817306\n",
      "[6, 1350] val loss: 3.951576\n",
      "[6, 1500] train loss: 3.806052\n",
      "[6, 1500] val loss: 3.820882\n",
      "[6, 1650] train loss: 3.832619\n",
      "[6, 1650] val loss: 3.788025\n",
      "[6, 1800] train loss: 3.812355\n",
      "[6, 1800] val loss: 4.098394\n",
      "[6, 1950] train loss: 3.825547\n",
      "[6, 1950] val loss: 3.796001\n",
      "[6, 2100] train loss: 3.829467\n",
      "[6, 2100] val loss: 3.900798\n",
      "[6, 2250] train loss: 3.822166\n",
      "[6, 2250] val loss: 3.643999\n",
      "[6, 2400] train loss: 3.803920\n",
      "[6, 2400] val loss: 3.613984\n",
      "[6, 2550] train loss: 3.790421\n",
      "[6, 2550] val loss: 4.053491\n",
      "[6, 2700] train loss: 3.811017\n",
      "[6, 2700] val loss: 3.988180\n",
      "[6, 2850] train loss: 3.799919\n",
      "[6, 2850] val loss: 3.820772\n",
      "[6, 3000] train loss: 3.818015\n",
      "[6, 3000] val loss: 3.790055\n",
      "[6, 3150] train loss: 3.778628\n",
      "[6, 3150] val loss: 4.021738\n",
      "[6, 3300] train loss: 3.848901\n",
      "[6, 3300] val loss: 3.941540\n",
      "[6, 3450] train loss: 3.834614\n",
      "[6, 3450] val loss: 3.919055\n",
      "[6, 3600] train loss: 3.789184\n",
      "[6, 3600] val loss: 3.856269\n",
      "[6, 3750] train loss: 3.851682\n",
      "[6, 3750] val loss: 3.840069\n",
      "[6, 3900] train loss: 3.807166\n",
      "[6, 3900] val loss: 3.767268\n",
      "[6, 4050] train loss: 3.789833\n",
      "[6, 4050] val loss: 3.600548\n",
      "[6, 4200] train loss: 3.820570\n",
      "[6, 4200] val loss: 3.978747\n",
      "[6, 4350] train loss: 3.838440\n",
      "[6, 4350] val loss: 4.072823\n",
      "[6, 4500] train loss: 3.797741\n",
      "[6, 4500] val loss: 3.801636\n",
      "[6, 4650] train loss: 3.807253\n",
      "[6, 4650] val loss: 3.704656\n",
      "[6, 4800] train loss: 3.810849\n",
      "[6, 4800] val loss: 4.138279\n",
      "[6, 4950] train loss: 3.816878\n",
      "[6, 4950] val loss: 3.623070\n",
      "[6, 5100] train loss: 3.803071\n",
      "[6, 5100] val loss: 3.823198\n",
      "[6, 5250] train loss: 3.803523\n",
      "[6, 5250] val loss: 3.945775\n",
      "[6, 5400] train loss: 3.791634\n",
      "[6, 5400] val loss: 3.759146\n",
      "[6, 5550] train loss: 3.801010\n",
      "[6, 5550] val loss: 3.747992\n",
      "[6, 5700] train loss: 3.780722\n",
      "[6, 5700] val loss: 3.855783\n",
      "[6, 5850] train loss: 3.806650\n",
      "[6, 5850] val loss: 3.800145\n",
      "[6, 6000] train loss: 3.807704\n",
      "[6, 6000] val loss: 3.791321\n",
      "[6, 6150] train loss: 3.817751\n",
      "[6, 6150] val loss: 4.035607\n",
      "[6, 6300] train loss: 3.826633\n",
      "[6, 6300] val loss: 3.842817\n",
      "[6, 6450] train loss: 3.821793\n",
      "[6, 6450] val loss: 3.836625\n",
      "[6, 6600] train loss: 3.824413\n",
      "[6, 6600] val loss: 3.717331\n",
      "[6, 6750] train loss: 3.806514\n",
      "[6, 6750] val loss: 3.878772\n",
      "[6, 6900] train loss: 3.811292\n",
      "[6, 6900] val loss: 3.935691\n",
      "[6, 7050] train loss: 3.799792\n",
      "[6, 7050] val loss: 4.028915\n",
      "[6, 7200] train loss: 3.803040\n",
      "[6, 7200] val loss: 3.842828\n",
      "[6, 7350] train loss: 3.801577\n",
      "[6, 7350] val loss: 3.807801\n",
      "[6, 7500] train loss: 3.806991\n",
      "[6, 7500] val loss: 3.724364\n",
      "[6, 7650] train loss: 3.795022\n",
      "[6, 7650] val loss: 3.594844\n",
      "[6, 7800] train loss: 3.800669\n",
      "[6, 7800] val loss: 3.770036\n",
      "[6, 7950] train loss: 3.819141\n",
      "[6, 7950] val loss: 3.595960\n",
      "[6, 8100] train loss: 3.799773\n",
      "[6, 8100] val loss: 3.888401\n",
      "[6, 8250] train loss: 3.799354\n",
      "[6, 8250] val loss: 4.054219\n",
      "[6, 8400] train loss: 3.834023\n",
      "[6, 8400] val loss: 3.872117\n",
      "[6, 8550] train loss: 3.820796\n",
      "[6, 8550] val loss: 3.883882\n",
      "[6, 8700] train loss: 3.821123\n",
      "[6, 8700] val loss: 3.957762\n",
      "[6, 8850] train loss: 3.826194\n",
      "[6, 8850] val loss: 3.984277\n",
      "[6, 9000] train loss: 3.805724\n",
      "[6, 9000] val loss: 3.835903\n",
      "[6, 9150] train loss: 3.851279\n",
      "[6, 9150] val loss: 3.905293\n",
      "[6, 9300] train loss: 3.823216\n",
      "[6, 9300] val loss: 3.837204\n",
      "[6, 9450] train loss: 3.839879\n",
      "[6, 9450] val loss: 3.935084\n",
      "[6, 9600] train loss: 3.820824\n",
      "[6, 9600] val loss: 3.692771\n",
      "[6, 9750] train loss: 3.794081\n",
      "[6, 9750] val loss: 3.872219\n",
      "[6, 9900] train loss: 3.818504\n",
      "[6, 9900] val loss: 3.919132\n",
      "[6,10050] train loss: 3.776390\n",
      "[6,10050] val loss: 4.081333\n",
      "[6,10200] train loss: 3.826995\n",
      "[6,10200] val loss: 3.897819\n",
      "[6,10350] train loss: 3.813476\n",
      "[6,10350] val loss: 3.763798\n",
      "[6,10500] train loss: 3.823885\n",
      "[6,10500] val loss: 3.992130\n",
      "[6,10650] train loss: 3.815296\n",
      "[6,10650] val loss: 3.919336\n",
      "[6,10800] train loss: 3.820878\n",
      "[6,10800] val loss: 4.032745\n",
      "[6,10950] train loss: 3.845656\n",
      "[6,10950] val loss: 3.801023\n",
      "[6,11100] train loss: 3.793518\n",
      "[6,11100] val loss: 3.818167\n",
      "[6,11250] train loss: 3.798751\n",
      "[6,11250] val loss: 3.840747\n",
      "[6,11400] train loss: 3.818517\n",
      "[6,11400] val loss: 3.996403\n",
      "[6,11550] train loss: 3.787380\n",
      "[6,11550] val loss: 3.922639\n",
      "[6,11700] train loss: 3.817673\n",
      "[6,11700] val loss: 3.633866\n",
      "[6,11850] train loss: 3.797868\n",
      "[6,11850] val loss: 3.944743\n",
      "[6,12000] train loss: 3.807136\n",
      "[6,12000] val loss: 3.891352\n",
      "[6,12150] train loss: 3.809786\n",
      "[6,12150] val loss: 3.777923\n",
      "[6,12300] train loss: 3.850170\n",
      "[6,12300] val loss: 3.996599\n",
      "[6,12450] train loss: 3.817017\n",
      "[6,12450] val loss: 3.728513\n",
      "[6,12600] train loss: 3.813214\n",
      "[6,12600] val loss: 3.638699\n",
      "[6,12750] train loss: 3.782368\n",
      "[6,12750] val loss: 3.658714\n",
      "[6,12900] train loss: 3.798775\n",
      "[6,12900] val loss: 3.920867\n",
      "[6,13050] train loss: 3.797617\n",
      "[6,13050] val loss: 4.043633\n",
      "[6,13200] train loss: 3.819584\n",
      "[6,13200] val loss: 3.895437\n",
      "[6,13350] train loss: 3.827664\n",
      "[6,13350] val loss: 3.912482\n",
      "[6,13500] train loss: 3.793116\n",
      "[6,13500] val loss: 4.038986\n",
      "[6,13650] train loss: 3.826559\n",
      "[6,13650] val loss: 3.781987\n",
      "[6,13800] train loss: 3.802685\n",
      "[6,13800] val loss: 3.889182\n",
      "[6,13950] train loss: 3.797085\n",
      "[6,13950] val loss: 4.069642\n",
      "[6,14100] train loss: 3.805522\n",
      "[6,14100] val loss: 3.923191\n",
      "[6,14250] train loss: 3.833030\n",
      "[6,14250] val loss: 3.787887\n",
      "[6,14400] train loss: 3.798205\n",
      "[6,14400] val loss: 3.887437\n",
      "[6,14550] train loss: 3.818588\n",
      "[6,14550] val loss: 3.741184\n",
      "[6,14700] train loss: 3.830832\n",
      "[6,14700] val loss: 3.856650\n",
      "[6,14850] train loss: 3.836499\n",
      "[6,14850] val loss: 4.099196\n",
      "[6,15000] train loss: 3.813019\n",
      "[6,15000] val loss: 3.868096\n",
      "[6,15150] train loss: 3.832387\n",
      "[6,15150] val loss: 3.725092\n",
      "[6,15300] train loss: 3.797539\n",
      "[6,15300] val loss: 3.798499\n",
      "[6,15450] train loss: 3.802458\n",
      "[6,15450] val loss: 3.450008\n",
      "[6,15600] train loss: 3.815521\n",
      "[6,15600] val loss: 3.652032\n",
      "[6,15750] train loss: 3.799365\n",
      "[6,15750] val loss: 3.814489\n",
      "[6,15900] train loss: 3.809446\n",
      "[6,15900] val loss: 3.884997\n",
      "[6,16050] train loss: 3.807042\n",
      "[6,16050] val loss: 3.663334\n",
      "[6,16200] train loss: 3.809238\n",
      "[6,16200] val loss: 3.551135\n",
      "[6,16350] train loss: 3.818738\n",
      "[6,16350] val loss: 3.925505\n",
      "[6,16500] train loss: 3.814499\n",
      "[6,16500] val loss: 3.701433\n"
     ]
    }
   ],
   "source": [
    "model=Transformer(embed_size,num_heads,dim_key,dim_value,dim_inner,layer_num,src_vocab_size,tgt_vocab_size).to(device)\n",
    "criterian=nn.CrossEntropyLoss(ignore_index=0) \n",
    "optimizer=Adam(filter(lambda p: p.requires_grad, model.parameters()),lr=learning_rate,betas=Betas_range,eps=eps_value)\n",
    " \n",
    "epochs=20\n",
    " \n",
    "train_loss_lst=[] #pe: per 10 iteration\n",
    " \n",
    "val_loss_lst=[] #pe: per epoch\n",
    "\n",
    "it_sz=150\n",
    "\n",
    "nn=750\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  running_loss=0\n",
    "  n=0\n",
    "  tr_l_pe=0\n",
    "  for i,(XX,YY) in enumerate(train_generator):\n",
    "    n=n+1\n",
    "    XX=XX.to(device)\n",
    "    YY=YY.to(device)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    outputs=model(XX,YY[:,:-1])\n",
    "\n",
    "    output_dim=outputs.shape[-1]\n",
    "\n",
    "    outputs = outputs.contiguous().view(-1, output_dim)\n",
    "    YY = YY[:,1:].contiguous().view(-1)\n",
    " \n",
    "    loss=criterian(outputs,YY)\n",
    " \n",
    "    loss.backward()\n",
    "\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 5)\n",
    " \n",
    "    optimizer.step()\n",
    " \n",
    "    running_loss+=loss.item()\n",
    "\n",
    "    \n",
    "    if i%it_sz==(it_sz-1):\n",
    "      print('[%d,%5d] train loss: %f'%(epoch+1,i+1,running_loss/it_sz))\n",
    "      train_loss_lst.append(running_loss/it_sz)\n",
    "      running_loss=0\n",
    "\n",
    "      running_loss_val=0\n",
    "      n1=0\n",
    "      for j,(XX_val,YY_val) in enumerate(val_generator):\n",
    "        n1=n1+1\n",
    "        XX_val=XX_val.to(device)\n",
    "        YY_val=YY_val.to(device)\n",
    "\n",
    "        outputs_val=model(XX_val,YY_val[:,:-1])\n",
    "\n",
    "        output_dim_val=outputs_val.shape[-1]\n",
    "\n",
    "        outputs_val = outputs_val.contiguous().view(-1, output_dim_val)\n",
    "        YY_val = YY_val[:,1:].contiguous().view(-1)\n",
    "    \n",
    "        loss_val=criterian(outputs_val,YY_val)\n",
    "\n",
    "        running_loss_val+=loss_val.item()\n",
    "        if n1==10:\n",
    "          break\n",
    "      print('[%d,%5d] val loss: %f'%(epoch+1,i+1,running_loss_val/n1))\n",
    "      val_loss_lst.append(running_loss_val/n1)\n",
    "      if len(val_loss_lst)==nn:\n",
    "        break\n",
    "    if len(val_loss_lst)==nn:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "JB1BcUGSegEE",
    "outputId": "e1b33e26-cd96-48b7-ab3d-296b580f5669"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f6f4bab6050>"
      ]
     },
     "execution_count": 17,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plotting loss\n",
    "plt.figure()\n",
    "itr_lst=list(range(1,len(val_loss_lst)+1))\n",
    "plt.plot(itr_lst,train_loss_lst)\n",
    "plt.plot(val_loss_lst)\n",
    "plt.xlabel('iterations(*150)')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['train loss','val loss'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "SkrxEgz0qf9A",
    "outputId": "f060f7c5-3459-4090-d20e-5f6accfd6299"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f6f4bb11990>"
      ]
     },
     "execution_count": 18,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plotting loss\n",
    "plt.figure()\n",
    "itr_lst=list(range(1,len(val_loss_lst)+1))\n",
    "plt.plot(itr_lst,train_loss_lst)\n",
    "plt.xlabel('iterations(*150)')\n",
    "plt.ylabel('loss')\n",
    "plt.legend(['train loss'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "w6aFwbruP7d2"
   },
   "outputs": [],
   "source": [
    "def lines_and_tokenized(path,tokenizer):\n",
    "  f=open(path,'r')\n",
    "  lines=f.readlines()\n",
    "\n",
    "  list_1=[]\n",
    "  list_2=[]\n",
    "  for line in lines:\n",
    "    line=tokenizer.tokenize(line)[0]\n",
    "    list_1.append(line)\n",
    "    line=\" \".join(line)\n",
    "    line=line.replace(\"_ \",\"\")\n",
    "    line=line.replace(\" _\",\"\")\n",
    "    line=line.replace(\"_\",\"\")\n",
    "    list_2.append(line)\n",
    "\n",
    "  return list_2,list_1\n",
    "\n",
    "\n",
    "original_en_path='/content/Test/test.en'\n",
    "org_lines,org_lines_tokenized=lines_and_tokenized(original_en_path,tokenizer_en)\n",
    "\n",
    "ref0_path='/content/Test/test.fa0'\n",
    "ref0_lines,ref0_lines_tokenized=lines_and_tokenized(ref0_path,tokenizer_fa)\n",
    "\n",
    "ref1_path='/content/Test/test.fa1'\n",
    "ref1_lines,ref1_lines_tokenized=lines_and_tokenized(ref1_path,tokenizer_fa)\n",
    "\n",
    "ref2_path='/content/Test/test.fa2'\n",
    "ref2_lines,ref2_lines_tokenized=lines_and_tokenized(ref2_path,tokenizer_fa)\n",
    "\n",
    "ref3_path='/content/Test/test.fa3'\n",
    "ref3_lines,ref3_lines_tokenized=lines_and_tokenized(ref3_path,tokenizer_fa)\n",
    "\n",
    "refs=[]\n",
    "for kk in range(len(org_lines)):\n",
    "  refs.append([ref0_lines_tokenized[kk],ref1_lines_tokenized[kk],ref2_lines_tokenized[kk],ref3_lines_tokenized[kk]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "greedy search credited to \n",
    "https://colab.research.google.com/github/bentrevett/pytorch-seq2seq/blob/master/6%20-%20Attention%20is%20All%20You%20Need.ipynb#scrollTo=IGGB4rUy_0zx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "3AVf2x83ehZm"
   },
   "outputs": [],
   "source": [
    "def greedy_search(test_data,model,max_len):\n",
    "  test_data=test_data.to(device)\n",
    "\n",
    "  mask_encoder=model.build_source_mask(test_data)\n",
    "  encoded=model.encoder(test_data,mask_encoder)\n",
    "  \n",
    "  memory_p_mask=model.build_pad_mask(test_data)\n",
    "\n",
    "  target_indices=[1]\n",
    "  for i in range(max_len):\n",
    "    test_y=torch.LongTensor(target_indices).unsqueeze(0).to(device)\n",
    "\n",
    "    target_mask=model.build_target_mask(test_y)\n",
    "    \n",
    "    decoder_dic={'tgt_mask':target_mask,'memory_mask':None,'tgt_key_padding_mask':None, 'memory_key_padding_mask':memory_p_mask}\n",
    "    decoded=model.decoder(test_y,encoded,decoder_dic)\n",
    "    out=model.fc(decoded)\n",
    "\n",
    "    predicted=out.argmax(2)[:,-1].item()\n",
    "\n",
    "    target_indices.append(predicted)\n",
    "\n",
    "    if predicted==2:\n",
    "      break\n",
    "  return target_indices\n",
    "\n",
    "def translate(the_tokens_list,the_vocabulary):\n",
    "  translated = [the_vocabulary.num2word[i] for i in the_tokens_list]\n",
    "  return translated[1:-1]\n",
    "\n",
    "\n",
    "def to_num(a_sen,a_vocabulary):\n",
    "  a_sen_2num=[1]+a_vocabulary.sentence_to_numbers(a_sen)+[2]\n",
    "  a_sen_2num=torch.tensor(a_sen_2num).unsqueeze(0)\n",
    "  return a_sen_2num\n",
    "\n",
    "def translated_lines(model,source_lines,source_vocabulary,target_vocabulary,max_len): \n",
    "  fa_translated=[]\n",
    "  for line in source_lines:\n",
    "    line=to_num(line,source_vocabulary)\n",
    "    line=greedy_search(line,model,max_len)\n",
    "    line=translate(line,target_vocabulary)\n",
    "    fa_translated.append(line)\n",
    "  return fa_translated\n",
    "\n",
    "def whole_sen(tokenized_sen_list):\n",
    "  a_list=[]\n",
    "  for line in tokenized_sen_list:\n",
    "    line=\" \".join(line)\n",
    "    line=line.replace(\"_ \",\"\")\n",
    "    line=line.replace(\" _\",\"\")\n",
    "    line=line.replace(\"_\",\"\")\n",
    "    a_list.append(line)\n",
    "  return a_list\n",
    "\n",
    "f=open(original_en_path,'r')\n",
    "org_lines_test=f.readlines()\n",
    "f.close()\n",
    "\n",
    "fa_translated_tokenized=translated_lines(model,org_lines_test,English_Vocabulary,Farsi_Vocabulary,150)\n",
    "fa_translated_lines=whole_sen(fa_translated_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "ZC-w_NDBb-t4"
   },
   "outputs": [],
   "source": [
    "def print_lines(list1,list2,list3,k):\n",
    "  print(list1[k])\n",
    "  print(list2[k])\n",
    "  print(list3[k])\n",
    "  print('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OMDwAUCAjYEv",
    "outputId": "2aa056fb-5aa8-43c8-90f9-6247584fb458"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "do you go by car and i go by train ?\n",
      "آیا شما با اتومبیل میروید و من با قطار میروم ؟\n",
      "آیا شما با ماشین قطار می‌روم و با ماشین می‌روم ?\n",
      "\n",
      "\n",
      "what did you say , please ?\n",
      "شما چه گفتید ، لطفا ؟\n",
      "لطفا , چه چیزی انجام می‌دهید ?\n",
      "\n",
      "\n",
      "yes . when and where do we want to meet ?\n",
      "بله . پس کی و کجا باید یکدیگر را ملاقات کنیم ؟\n",
      "و چه زمانی ما میتوانیم همدیگر را ملاقات کنیم ?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_lines(org_lines,ref1_lines,fa_translated_lines,2)\n",
    "\n",
    "print_lines(org_lines,ref0_lines,fa_translated_lines,8)\n",
    "\n",
    "print_lines(org_lines,ref0_lines,fa_translated_lines,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kmLe4hA_k9tg",
    "outputId": "9a283717-bf3a-4333-8181-5dee71eb539d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i prefer the plane .\n",
      "من هواپیما را ترجیح میدهم .\n",
      "من ترجیح می‌دهم هواپیما را ترجیح میدهم .\n",
      "\n",
      "\n",
      "no idea . we will see . it does not matter .\n",
      "نظری ندارم . یکدیگر را میبینیم . مهم نیست .\n",
      "ما هیچ ایده ای نمی‌بینیم .\n",
      "\n",
      "\n",
      "what is planned for the evening ?\n",
      "برای عصر چه برنامه­ای گذاشته شده است ؟\n",
      "عصر برای عصر برنامه-ریزی شده است ?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_lines(org_lines,ref1_lines,fa_translated_lines,16)\n",
    "\n",
    "print_lines(org_lines,ref0_lines,fa_translated_lines,18)\n",
    "\n",
    "print_lines(org_lines,ref2_lines,fa_translated_lines,23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ic4wJzO9mF2S",
    "outputId": "b94604f8-8db3-4e3f-fb8d-c65735e99eed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fine . i think we have arranged everything . then we will meet tomorrow .\n",
      "خوبه . من فکر میکنم ما ترتیب همه چیز را دادیم . پس ما فردا ملاقات خواهیم کرد .\n",
      "پس من فکر میکنم که همه چیز را ملاقات کنیم . پس ما باید خوب را ملاقات کنیم .\n",
      "\n",
      "\n",
      "hello . we have to talk about our trip to hanover .\n",
      "سلام . ما باید درمورد سفرمان به هانوفر صحبت کنیم .\n",
      "سلام به سفر ما باید درباره سفر کنیم .\n",
      "\n",
      "\n",
      "i have already booked a flight .\n",
      "من یک پرواز را رزرو کرده­ام .\n",
      "من یک پرواز رزرو کردم .\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_lines(org_lines,ref1_lines,fa_translated_lines,25)\n",
    "\n",
    "print_lines(org_lines,ref1_lines,fa_translated_lines,26)\n",
    "\n",
    "print_lines(org_lines,ref1_lines,fa_translated_lines,28)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rt6GhV4Bm8Cz",
    "outputId": "4b9b4a38-150b-4c65-a251-75e169b1d925"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we will have to meet at the airport at seven o'clock .\n",
      "ما باید ساعت هفت در فرودگاه ملاقات کنیم .\n",
      "ما باید هفت ساعت هفت ملاقات کنیم .\n",
      "\n",
      "\n",
      "the best thing is we meet at the train station at eight o'clock .\n",
      "بهترین چیز این است که ما شاعت هشت در ایستگاه قطار ملاقات کنیم .\n",
      "ساعت هشت چیز در بهترین زمینه آموزش‌های قطار است .\n",
      "\n",
      "\n",
      "goodbye .\n",
      "خداحافظ .\n",
      "خداحافظ .\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_lines(org_lines,ref1_lines,fa_translated_lines,30)\n",
    "\n",
    "print_lines(org_lines,ref1_lines,fa_translated_lines,36)\n",
    "\n",
    "print_lines(org_lines,ref1_lines,fa_translated_lines,39)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7E3gd8sZnpXb",
    "outputId": "05e10163-33c3-4efa-b32e-513abba17171"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "okay . should we go to the hotel by taxi immediately ?\n",
      "باشه . ما باید فورا با تاکسی به هتل برویم ؟\n",
      "بسیار خوب باید با راننده تاکسی به هتل برویم .\n",
      "\n",
      "\n",
      "okay . should we meet at the hotel at half past seven ?\n",
      "باشه . ما باید هفت و نیم در هتل ملاقات کنیم ؟\n",
      "خوب ما باید ساعت هفت در هتل ملاقات کنیم ?\n",
      "\n",
      "\n",
      "good morning . how are you ?\n",
      "صبح به­خیر . حال شما چطور است ؟\n",
      "خوب , صبح خوب است ?\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_lines(org_lines,ref1_lines,fa_translated_lines,101)\n",
    "\n",
    "print_lines(org_lines,ref1_lines,fa_translated_lines,150)\n",
    "\n",
    "print_lines(org_lines,ref1_lines,fa_translated_lines,180)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GflUfSb4oU2s",
    "outputId": "b61dddd9-79f6-4ec5-97ba-80e13cf04236"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i did not understand that .\n",
      "من آن را نفهمیدم .\n",
      "آن را درک نمی‌کنم .\n",
      "\n",
      "\n",
      "unfortunately i did not understand you .\n",
      "متاسفانه منظورت را نفهمیدم .\n",
      "متأسفانه من شما را درک نمی‌کنم .\n",
      "\n",
      "\n",
      "i prefer the plane .\n",
      "من هواپیما را ترجیح میدهم .\n",
      "من هواپیما را ترجیح دادم .\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_lines(org_lines,ref0_lines,fa_translated_lines,65)\n",
    "\n",
    "print_lines(org_lines,ref3_lines,fa_translated_lines,131)\n",
    "\n",
    "print_lines(org_lines,ref3_lines,fa_translated_lines,110)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6g_VxkcvpJVK",
    "outputId": "0e87dcb3-bbd5-4748-cb82-a068bfedc7e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU is 0.1075225802187754\n",
      "NIST is 0.0009707774813979547\n"
     ]
    }
   ],
   "source": [
    "bleu_score=corpus_bleu(refs,fa_translated_tokenized)\n",
    "print('BLEU is '+str(bleu_score))\n",
    "nist_score=corpus_nist(refs,fa_translated_tokenized)\n",
    "print('NIST is '+str(nist_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "TeItQuZ2Y1h2"
   },
   "outputs": [],
   "source": [
    "torch.save(model,'/content/model')\n",
    "torch.save(model,'/content/drive/MyDrive/DL_HW4/Q2_2/model_whole')\n",
    "torch.save(model.state_dict(), '/content/drive/MyDrive/DL_HW4/Q2_2/model')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Q2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
